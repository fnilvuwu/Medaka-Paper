% \subsection{Data Sources}
% The dataset used in this study consisted of both primary and secondary images of Medaka fish. Primary data were obtained by directly photographing \textit{Oryzias javanicus} and \textit{Oryzias celebensis} in an aquarium using a digital camera. Secondary data were collected from publicly available sources, including research websites, aquatic forums, and open-access databases. The final dataset contained 792 images after curation, covering a variety of lighting conditions and viewing angles to enhance robustness and generalization. All annotated datasets and code used in this study will be made publicly available in an online repository upon publication. Since only non-invasive aquarium photography was conducted, no ethical approval was required for animal experimentation. 

% \subsection{Data Preprocessing}
% All images were standardized in orientation, resolution, and color balance. Image restoration (contrast enhancement, artifact removal), data augmentation (flipping, rotations of $90^{\circ}$, $180^{\circ}$, and $270^{\circ}$), and normalization were applied. Annotation was performed using the Roboflow platform, with bounding boxes and class labels stored in YOLO format. The dataset was split into training, validation, and test sets with proportions of 80\%, 10\%, and 10\%, respectively.

% \subsection{Model Training}
% We fine-tuned the YOLOv8 architecture on the annotated dataset. To evaluate robustness, 5-fold cross-validation was employed. For ensemble learning, multiple YOLOv8 models with different weight initializations were combined using Weighted Box Fusion (WBF). Training and evaluation were performed on an NVIDIA GPU with default YOLOv8 hyperparameters unless otherwise specified. The implementation was carried out in Python using the Ultralytics YOLOv8 framework.

% \subsection{Evaluation Metrics}
% Model performance was assessed using COCO metrics, including mean Average Precision (mAP@0.5:0.95), precision, recall, and bounding box regression error (RMSE). Confusion matrices and loss curves were also analyzed to assess classification performance and model convergence.

% \subsection{Use of Generative AI}
% Generative artificial intelligence (ChatGPT, OpenAI) was used solely to improve the clarity and language of the manuscript. No AI tools were used to generate data, perform analyses, or influence interpretation of results.

\subsection{Data Sources}
The dataset employed in this study comprised both primary and secondary image data of Medaka fish. Primary data were obtained through direct aquarium photography of \textit{Oryzias javanicus} and \textit{Oryzias celebensis} using a digital camera. Secondary data were collected from publicly accessible sources, including research websites, aquatic community forums, and open-access databases. After curation, the final dataset consisted of 792 images that captured diverse lighting conditions and viewing angles, thereby improving robustness and generalization. All annotated data and code will be made publicly available in an online repository upon publication. Since data collection relied solely on non-invasive aquarium photography, no ethical approval was required.

\subsection{Data Preprocessing}
To ensure consistency, all images were standardized in orientation, resolution, and color balance prior to training. Preprocessing included data normalization along with augmentation strategies to enhance robustness and reduce overfitting. These augmentations involved adjustments to color properties such as hue, saturation, and brightness to simulate varying lighting and environmental conditions; translations and scaling to represent objects at different positions and distances; and horizontal flipping to increase dataset diversity. The mosaic technique was also applied, combining four images into a single training instance to expose the model to more complex scene compositions and object interactions. Furthermore, random erasing of image regions was employed to encourage the model to identify less obvious but relevant features. Annotation was carried out using the Roboflow platform, with bounding boxes and class labels stored in YOLO format. The dataset was subsequently divided into training (80\%), validation (10\%), and testing (10\%) subsets.

\subsection{Model Training}
The YOLOv8 architecture was fine-tuned on the annotated dataset. To evaluate model robustness, 5-fold cross-validation was implemented. For ensemble learning, multiple YOLOv8 models initialized with different weights were combined using Weighted Box Fusion (WBF), which integrates predictions by considering both bounding box overlap and confidence scores. Training and evaluation were carried out on an NVIDIA GPU environment, following the default hyperparameters of the YOLOv8 framework unless specified otherwise. The implementation was conducted in Python using the Ultralytics YOLOv8 framework.

\subsection{Evaluation Metrics}
Model performance was assessed using the COCO evaluation metrics, including mean Average Precision (mAP@0.5:0.95), precision, recall, and bounding box regression error (RMSE). Additional analyses, such as confusion matrices and training loss curves, were used to further evaluate classification performance and convergence.
