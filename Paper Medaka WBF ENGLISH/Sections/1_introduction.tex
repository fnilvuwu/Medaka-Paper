The advancement of artificial intelligence (AI) has significantly contributed to wildlife conservation efforts, particularly in automating species identification and monitoring. These technologies are invaluable in environments where manual observation is difficult, such as underwater ecosystems, where factors like turbidity and low light complicate visibility \citep{LeCun2015, Liu2019}. AI-based systems can process extensive volumes of visual data from images and videos to assist in detecting species presence, tracking their behavior, and supporting conservation strategies with minimal human intervention \citep{Kalafi2018}.

In this context, object detection models have gained traction for their speed and efficiency in real-time applications. The You Only Look Once (YOLO) architecture, particularly its latest version, YOLOv8, enhances prior iterations with improved accuracy and architectural efficiency, making it suitable for challenging tasks like aquatic species recognition. YOLOv8 effectively counters issues surrounding image degradation commonly faced underwater, such as turbidity, unfavorable lighting, and occlusion \citep{Redmon2017}.

Our study specifically targets the detection of two notable Medaka fish species—\textit{Oryzias javanicus} and \textit{Oryzias celebensis}. These species pose unique monitoring challenges due to their dwindling populations and the complexities of underwater imaging. To bolster this investigation, we constructed a custom dataset comprising both manually collected images using cameras and publicly available images from internet resources. This dataset contains a total of 2,016 images, segregated into 1,857 images (92\%) for training and 159 images (8\%) for testing. We utilized Roboflow for manual annotation of the dataset, alongside conducting essential preprocessing and augmentation \citep{Dang2020}.

The preprocessing steps involved automatic orientation adjustments, resizing images to a standard 640×640 pixels, and filtering out any empty annotations. To enhance the robustness of the model and improve generalization, augmentations such as horizontal and vertical flips, as well as rotations (90°, 180°, and 270°), were applied \citep{Arbogast2016}. These methodological choices mitigate the risks of overfitting and aim to simulate various real-world conditions that the model may encounter.

For the detection task, we employed a YOLOv8-based approach featuring 5-fold cross-validation to train five individual models. Predictions from these distinct models were subsequently refined and combined using Weighted Box Fusion (WBF), an advanced ensemble method that enhances final bounding boxes by evaluating confidence scores and overlaps—ultimately bolstering the model's overall detection performance \citep{Solovyev2021, Leow2015}. Our findings indicate that this ensemble approach achieves a substantial increase in mean Average Precision (mAP) relative to a single YOLOv8 model. Notably, while ensemble methods do increase inference time, they significantly enhance the model's robustness, especially when detecting small or less visible targets such as Medaka fish. Through this research, we establish a foundational reference for the development of lightweight ensemble methodologies applicable to underwater object detection, utilizing advanced real-time detection models \citep{Salimi2016}. 