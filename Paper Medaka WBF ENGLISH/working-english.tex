%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{YOLOv8-WBF: Ensemble Learning for Reliable
Detection of Endangered Medaka (Oryzias)}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Title}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0000-0000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{
Rahmatullah R. $^{1,\orcidA{}}$,
Armin Lawi $^{1, 2, 3}$, 
Muhammad Haerul $^1$,
Iman Mustika Ismail $^1$,
Irma Andriani $^4$, 
Andi Iqbal Burhanuddin $^5$, and 
Mario Köppen $^6$
}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Armin Lawi, Irma Andriani, Andi Iqbal Burhanuddin and Mario Koeppen}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Information Systems Study Program, Faculty of Mathematics and Natural Sciences, Hasanuddin University, Indonesia \\
$^{2}$ \quad Data Science and Artificial Intelligence Research Group, Hasanuddin University, Indonesia \\
$^{3}$ \quad B.J. Habibie Institute of Technology, Parepare, Indonesia \\
$^{4}$ \quad Department of Biology, Faculty of Mathematics and Natural Sciences, Hasanuddin University, Indonesia \\
$^{5}$ \quad Department of Fishery, Faculty of Fishery and Marine Sciences, Hasanuddin University, Indonesia \\
$^{6}$ \quad Department of Creative Informatics, Faculty of Computer Science and Systems Engineering, Kyushu Institute of Technology, Japan}

% Contact information of the corresponding author
\corres{Correspondence : armin@unhas.ac.id)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Reliable detection of Medaka (Oryzias) fish is essential for ecological monitoring and conservation, particularly for tracking population trends of endangered species. This study evaluates the performance of a state-of-the-art deep learning model (YOLOv8) and an ensemble approach using Weighted Box Fusion (WBF) on a manually annotated dataset of Medaka images collected from online sources. Models were trained and validated using 5-fold cross-validation, and performance was assessed using COCO metrics, including mean Average Precision (mAP), precision, recall, and bounding box regression error. The YOLOv8-WBF ensemble achieved a mAP@0.5:0.95 of 0.578, representing an 8\% improvement over the best single model. It also enhanced bounding box localization and classification reliability, particularly for small and visually challenging fish instances. These accuracy gains came at the expense of computational efficiency, with inference requiring approximately five times more operations than a single YOLOv8 model. While less suited for real-time deployment, the ensemble approach offers more reliable detection for offline ecological workflows, where accuracy is prioritized over speed. By reducing missed detections of rare or occluded fish, this work contributes to more robust biodiversity monitoring and provides a baseline for developing optimized ensemble and lightweight detection models in aquatic conservation.}

% Keywords
\keyword{Medaka (Oryzias); Deep Learning; Object Detection; YOLOv8; Weighted Box Fusion; Ensemble Learning; Ecological Monitoring; Biodiversity Conservation} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine, Future, Sensors and Smart Cities
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent This is an obligatory section in ``Advances in Respiratory Medicine'', ``Future'', ``Sensors'' and ``Smart Cities”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Medaka fish (Oryzias) are small freshwater fish valued as ornamental species and are significant for biodiversity studies. They are species that are under danger of extinction as declared by the International Union for Conservation of Nature (IUCN), which makes protecting them an ecological priority. Identifying Medaka apart is challenging since they have subtle morphological differences and small sizes. Not only that, but the variability of aquatic environments also makes it even more challenging. Traditionally, medaka fish have been sacrificed in taxonomy studies due to their genetic value for lineage studies ~\cite{Mahmudi2022}. Their body colour serves as a social signal and reflects environmental conditions~\cite{RapidBodyColourationOryzias2020}, making them potential indicators of ecosystem health. Research conducted using traditional methods, such as direct capture and then being put in an experimental tank to understand its anatomy and internal structure, can harm the organism and limit its application in long-term ecological and genetic studies. Reliable detection technologies can facilitate non-invasive conservation without the need for traditional conservation methods. This technique protects species and ecosystems while enabling effective ecological monitoring in conservation efforts.

Deep learning has evolved beyond digit identification to sophisticated object detection, facilitating applications in autonomous vehicles, medical diagnostics, agricultural automation, and environmental monitoring~\cite{LeCun2015,Zhao2019}.In ecological research, object detection has been utilized for monitoring insects in agriculture~\cite{Tang2023,Ciampi2023} and detecting wildlife in natural habitats~\cite{Roy2023,Wenhan2024,Sun2024}, where precise identification is frequently challenging due to visual similarities among species and the complexity of water environments. These issues are also present in the medaka fish species, such as Oryzias javanicus and Oryzias celebensis, which have subtle morphological distinctions that complicate the reliable identification of endemic fish.

The YOLO architecture is one of the most widely used deep learning-based one-stage object detectors. Among its versions, YOLOv8 has shown a particularly good balance between speed and accuracy, as demonstrated in comparative benchmarks that evaluated YOLOv8 through YOLOv11 under real-world conditions~\cite{Sapkota2024}. Researchers have also proposed many variants to push YOLO’s performance further, especially in challenging settings. For example, CEH-YOLO adds a high-order deformable attention (HDA) module to better highlight important spatial features, an Enhanced Spatial Pyramid Pooling-Fast (ESPPF) module for improved texture and color feature extraction, and a Composite Detection module to boost detection of small or overlapping underwater objects, along with using WIoU-v3 loss to improve bounding box regression under hard conditions~\cite{CEH-YOLO}. ~\cite{YOLO-SAG} introduces the Softplus activation function to improve training stability, an AIFI module to strengthen intra-scale feature interactions (reducing false positives and missed detections), and lightweight neck convolution modules (GSConv, VoV-GSCSP) to reduce computational overhead while maintaining accuracy. SCoralDet focuses on underwater soft coral detection, using a Multi-Path Fusion Block (MPFB) to handle varied scales and lighting colors, lightweight modules for efficiency, and an Adaptive Power Transformation label assignment strategy to better align anchors with ground truth when coral structures are complex or blurred~\cite{SCoralDet}. While these studies mainly alter the internal YOLO architecture to address trade-offs between accuracy and speed, in our work, we propose the use of an ensemble method that builds on YOLOv8 without modifying its core architecture by combining the strengths of multiple detection heads or models to improve reliability under environmental variability.

Despite its strong performance, YOLO models could still make misclassifications by detecting background regions as objects or producing duplicate overlapping predictions of the same object. By default, these redundant outputs are reduced through Non-Maximum Suppression (NMS), yet this technique has well-known limitations, particularly when objects overlap or when multiple plausible predictions exist. Recent studies have come up with a new ensemble method, Weighted Boxes Fusion (WBF), that provides a more accurate and robust alternative, since it merges bounding boxes based on confidence scores and spatial alignment rather than discarding valuable predictions outright~\cite{Solovyev2021}. The limitations of standard non-maximum suppression (NMS) underscore the need to investigate advanced fusion strategies in object detection. Ensemble methods are particularly effective because they combine multiple models or detection heads, thereby improving uncertainty management, reducing false detections, and increasing consistency in challenging scenarios. The primary contribution of this work is the development of an ensemble approach for YOLOv8 that incorporates Weighted Box Fusion (WBF) to improve detection reliability in ecological monitoring applications, where accuracy is essential and misclassification significantly affect conservation outcomes.

In this work, we propose a novel Medaka fish dataset with images captured under diverse lighting colors to provide a realistic and challenging benchmark for ecological monitoring. Building on this resource MEDAKA-$\tilde{\mathrm{e}}$L, an ensemble-based detection framework for accurate and reliable identification of Medaka fish. By training 5 different YOLOv8n models trained across a 5-fold cross-validation of the dataset, which causes the model to learn its own unique feature of the Medaka fish.  At the ensemble stage, predictions from multiple YOLOv8 models trained through cross-validation are combined using Weighted Boxes Fusion (WBF). It addresses challenges such as background misclassifications and redundant overlapping bounding boxes. Unlike traditional Non-Maximum Suppression (NMS), WBF merges bounding boxes based on confidence scores and spatial alignment, preserving valuable detections and reducing false positives that achieve higher accuracy. The key contributions of this work are as follows:

\begin{itemize}
    \item Introduced the MEDAKA-$\tilde{\mathrm{e}}$L framework that uses 5 models trained across 5-fold cross-validation, and combines their outputs with Weighted Boxes Fusion (WBF) to improve detection accuracy.

    \item Proposed the Medaka dataset, a new collection of Medaka fish images that includes manually captured photos under different lighting colors, as well as annotated samples gathered from the internet.

    \item Developed an ensemble detection approach based on YOLOv8n models, where predictions are merged with WBF to keep valuable detections and reduce false positives.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

\subsection{Data Gathering}
% Penjelasan tentang teknik pengambilan gambar, lokasi pengambilan, spek kamera untuk primary data. Sementara untuk secondary data, sumbernya dari mana, totalnya berapa. 

% Jelaskan karena medaka termasuk spesies yang langka, sehingga pengumpulan datanya itu dari laboratorium terkontrol yang sudah dikumpulkan semuanya.  Therefore, or

% Karena data yang ada di lab sedikit, dilakukan pula pengumpulan data dari internet sebagai secondary data.

% TODO: Putuskan datasetnya namanya mixture medaka fish dataset atau medaka fish dataset saja

Given the scarcity of publicly available Medaka fish imagery and the critical need for precise taxonomic identification in ecological monitoring, we constructed a comprehensive hybrid dataset called the \textit{Medaka Fish Dataset}. This dataset, as shown in Figure \ref{fig:data_acquisition_strategies}, combines primary data that we collected through controlled laboratory imaging and \textit{in situ} field observation, with curated internet-sourced images as our secondary data. This approach directly addresses the fundamental challenge specified in our introduction regarding the absence of dedicated datasets for endangered Medaka species detection while ensuring sufficient morphological diversity to support the robustness of our proposed model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Images/mixture-medaka-dataset.pdf}
    \caption{Representative samples from the Medaka dataset showing diversity in species, environmental conditions, and imaging scenarios. (\textbf{a}) \textit{O. celebensis} specimens in various naturalistic settings. (\textbf{b}) \textit{O. javanicus} specimens demonstrating morphological variation and environmental diversity.}
    \label{fig:data_acquisition_strategies}
\end{figure}

Primary data acquisition combined (i) controlled laboratory imaging at the Genetics Laboratory, Faculty of Mathematics and Natural Sciences, Hasanuddin University, Indonesia and (ii) \textit{in situ} observation in the Tanjung coastal freshwater-estuarine transition zone in Makassar, Indonesia.
% Penjelasan spek
This dual setting was intentionally designed to balance morphological clarity with real-world environmental variability. In the laboratory setting, \textit{O. celebensis} and \textit{O. javanicus} were photographed using a Canon EOS M50 camera (24.1 MP, 6000×4000 px CMOS sensor) in modular glass aquaria with four different background colors. The camera operates at an aperture of \textit{f/2.5} under diffuse LED lighting. This is done to help minimize light reflection and motion blur while maintaining natural colors. We also used four background colors (red, black, blue, and green) to (1) enhance contrast across pigmentation conditions, (2) avoid overfitting on single color-light pairs, and (3) approximate the natural substrate diversity.

In addition to controlled laboratory imaging, we also added \textit{in situ} observation images to increase the variety of our data. These field images contributed heterogeneous backgrounds, such as irregular substrates, floating particles, and fluctuating lighting. However, images affected by severe turbidity, glaring light reflections, occlusions, or unclear body contours were discarded. Nearly identical temporal sequences, excessive motion blur, and noticeable chromatic aberrations were also excluded. Only minor normalization was applied, consisting of orientation adjustments and removal of unusable frames. No denoising or color correction was performed in order to preserve the genuine visual variation.

In parallel, secondary images were curated from publicly available online sources. To ensure reliability, only specimens that were clear and taxonomically identifiable were retained. Also, images with artificial backgrounds, compression artifacts, or ambiguous species characteristics were excluded to maintain the integrity of our dataset.

% It can be seen from Figure \ref{fig:data_acquisition_strategies} that we use two strategies to build our dataset. Our primary data were taken from combining (i) controlled laboratory imaging (Genetics laboratory, Faculty of Mathematics and Natural Sciences, Hasanuddin University) and (ii) in-situ observation in the Tanjung coastal freshwater-estuarine transition zone (Makassar). Meanwhile, our secondary data was collected from various websites using Google Image crawling, and then we manually selected it to ensure the quality.

% 

% Penjelasan tentang dataset lebih rinci
% DONE cek ulang jumlah dataset asli sesaat setalah akuisisi selesai
% DONE mungkin perlu dieval dulu apakah harus langsung disebut di sini hasil anotasinya atau bagaimana
Consequently, the combination of the two acquisition strategies yielded a comprehensive dataset that balances controlled imaging precision with ecological variability. In total, 1,511 high-resolution images ranging from 1920x1080 to 4096x3072 pixels of Medaka fish (\textit{Oryzias} species) were compiled.

% \begin{table}[H]
% \caption{Dataset distribution of Medaka fish images.}
% \label{tab:data_sources}
% \begin{tabularx}{\textwidth}{lCCC}
% \toprule
% \textbf{Species} & \textbf{Primary Data} & \textbf{Secondary Data} & \textbf{Total}\\
% \midrule
% \textit{O. javanicus} & 257 & 178 & 435 \\
% \textit{O. celebensis} & 287 & 70  & 357 \\
% \midrule
% \textbf{Total} & \textbf{544} & \textbf{248} & \textbf{792} \\
% \bottomrule
% \end{tabularx}
% \end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/dataset_hybrid.jpg}
    \caption{Examples of annotated Medaka images from the mixed dataset.}
    \label{fig:dataset_hybrid}
\end{figure}

\subsection{Data Filtering, Pre-processing, and Annotation}

% The collected raw images underwent a series of filtering and preprocessing steps prior to annotation to ensure consistency and quality across the dataset.

% We first filtered the raw images based on file size and Peak Signal-to-Noise Ratio (PSNR) to eliminate corrupted or low-quality samples. Images smaller than X KB or with PSNR below Y dB were discarded. As a result, only N images (Z% of the total) were retained for further processing.

% Each image was auto-oriented to correct metadata-induced rotations and resized to 640×640 pixels to ensure compatibility with subsequent model training. No data augmentation was applied at this stage to preserve the natural distribution of visual features.

% Filtered and preprocessed images were then annotated using Roboflow. Bounding box annotations were generated for each object of interest following consistent labeling guidelines. The final annotations were exported in YOLO format, which directly integrates with the training pipeline.

Subsequently, as illustrated in Figure \ref{fig:data_acquisition_strategies}, the collected raw images and raw photos were filtered based on their file size and PSNR values. The PSNR for each image was computed using Equation~\ref{eq:psnr}, where the Mean Squared Error (MSE) between an image and its reference was obtained using Equation~\ref{eq:mse}.

\begin{equation}
    \label{eq:mse}
    MSE = \frac{1}{mn} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} [I(i,j) - K(i,j)]^2
\end{equation}

\begin{equation}
    \label{eq:psnr}
    PSNR= 10 \log_{10} \frac{MAX^2_i}{MSE}
\end{equation}

A higher PSNR indicates better image fidelity and lower noise. Hence, images falling below a predefined PSNR threshold were discarded to maintain dataset integrity. The remaining samples were then passed through a preprocessing pipeline.

There were two preprocessing steps applied in this research: (1) automatic orientation correction and (2) image resizing to a fixed resolution of 640x640 pixels. Both preprocessing steps follow the default configuration commonly adopted in YOLO-based object detection pipelines.

% After filtering the dataset, we preprocessed
% Preprocessing
% Auto-Orient: Applied
% Resize: Stretch to 640x640

% Then the Medaka fish in these images were annotated using Roboflow, resulting in a total of 1,280 labeled instances: 641 of \textit{Oryzias celebensis} and 639 of \textit{Oryzias javanicus}. 

% DONE: Cek ulang angkanya, hasil preprocessing, tiap label berapa, dan tiap image berapa objek

Finally, 1,139 filtered and preprocessed images were annotated using Roboflow, resulting in a total of 1,239 labeled instances: 641 of \textit{Oryzias celebensis} and 598 of \textit{Oryzias javanicus}, respectively. Figure \ref{fig:dataset_hybrid} shows the examples of annotated Medaka images from our dataset. The annotation process was conducted by domain experts, who applied bounding boxes to each fish instance and assigned the corresponding species class. In most cases, each image contained a single fish object (1,071 images), while 62 images included two to three objects, and only four images contained between four and five objects.

\subsection{Yolo's Architecture V8}

% Model training was conducted using the AdamW optimizer with an initial learning rate of 0.001, weight decay of 0.0005, and cosine annealing learning rate scheduling. Training proceeded for 300 epochs with early stopping based on validation mAP monitoring. Input images were resized to 640×640 pixels while maintaining aspect ratios through appropriate padding to preserve spatial relationships.

% DONE: Cek spesifikasi trainingnya
% DONE: Cek versi yolonya apa, apakah 8s atau 8 biasa
In this study, we used the YOLOv8 model as our foundation object detection model. Figure \ref{fig:gambar_yolo} shows the detailed architecture of YOLOv8 and its three main components, the backbone, neck, and head \cite{yaseen_what_2024}. The first component, the backbone, extracts multi-scale features from input images using an advanced convolutional neural network (CNN) that improves upon the previous CSPLayer from YOLOv5, now called the C2f module \cite{Terven2023}. This module applies depthwise separable convolution to optimize the balance between processing speed and feature extraction capabilities \cite{yaseen_what_2024}. The second component, the neck, integrates and refines the extracted features through the combined adaptation of the Path Aggregation Network (PANet) and the Feature Pyramid Network (FPN). This structure performs multi-scale feature fusion by aggregating feature maps from three hierarchical levels of the backbone using the C2f module \cite{wang_alf-yolo_2024}. Finally, the head component is responsible for generating the final prediction, which includes bounding box coordinates, confidence scores, and class labels using an anchor-free approach. This approach replaces the previous version's anchor-based method to simplify the prediction process, reduce the number of hyperparameters, and improve generalization abilities to variations in object size and proportions \cite{Terven2023}. Thus, by integrating these three components, YOLOv8 improves accuracy, inference speed, and flexibility for a range of object detection tasks.

%YOLOv8 improves upon Yolov5 through three key architectural changes: the C2f module, anchor-free detection, and a decoupled head \cite{Terven2023}. 

% DONE: Berapa batch sizenya

The training process was carried out on the Medaka dataset with an input size of 640×640 pixels for 50 epochs using the AdamW optimizer with a initial learning rate of 0.01, a batch size of 16, and weight decay of 0.05. Model performance was evaluated using the mean Average Precision (mAP) and mean Average Recall (mAR) to measure detection capabilities for each object class.

\begin{figure}[H]
    \centering
    \includegraphics[draft,width=8cm,height=5cm]{gambar-tidak-ada.png}
    \caption{Yolo V8 Architecture}
    \label{fig:gambar_yolo}
\end{figure}

\subsection{k-fold validation}
% K-fold bekerja dengan membagi dataset menjadi k fold 
k-fold validation is a technique used to estimate the performance of a learning algorithm on a given dataset. It works by randomly dividing a dataset into $k$ disjoint folds with approximately equal size. One fold is used as the validation set for the other $k-1$ folds used to train the algorithm in each iteration. The algorithm’s overall performance is then computed as the average of the evaluation metrics obtained across all $k$ folds, and thus reflects performance estimation at the fold level \cite{wong_performance_2015}.

% DONE: Kaitkan dengan penelitian sebelumnya yang memainkan versi dan size
Previous studies, such as \cite{Solovyev2021, khalili_face_2022}, have shown that box ensembling  indeed increases the performance of object detection models. However, these works mainly focused on ensembling different YOLO versions or model sizes, putting more attention on model diversity than data diversity. In contrast, we looked back at some traditional machine learning techniques, in which we identify opportunities to improve performance by focusing on the dataset itself, especially in cases of data scarcity, through $k$-fold cross-validation-based ensembling learning models.

In this study, a YOLOv8 model was applied to the Medaka dataset using a 5-fold cross-validation scheme. The dataset was divided into five subsets, each comprising 16\%, and one test subset comprising 20\%. In each fold iteration, one 16\% subset was used for validation, while the remaining four subsets were used for training, with the 20\% subset remaining as the fixed test set. These folds were rotated alternately so that each 16\% subset became the validation set once, resulting in five different fold configurations:

\begin{itemize}
    \item Fold 1: 16\% train, 16\% train, 16\% train, 16\% train, 16\% val, 20\% test
    \item Fold 2: 16\% train, 16\% train, 16\% train, 16\% val, 16\% train, 20\% test
    \item Fold 3: 16\% train, 16\% train, 16\% val, 16\% train, 16\% train, 20\% test
    \item Fold 4: 16\% train, 16\% val, 16\% train, 16\% train, 16\% train, 20\% test
    \item Fold 5: 16\% val, 16\% train, 16\% train, 16\% train, 16\% train, 20\% test
\end{itemize}

\subsection{Ensemble Methods} \label{sec: ensemble}
% TODO: Tambahkan penjelasan input ensemblenya
After applying 5-fold cross-validation, the predictions from the trained five YOLOv8 models were combined using ensemble techniques. We experimented with three ensemble methods commonly used in object detection: Weighted Box Fusion (WBF), Non-Maximum Suppression (NMS), and Non-Maximum Weighted (NMW). In our experiments, we evaluated these three methods with IoU threshold of $\theta = 0.5, 0.75$, and a range of $0.50:0.95$ using MaxDets of 1, 10, and 100 accross different object scales.

\subsubsection{Non-Maximum Suppression (NMS)}
In object detection, Non-Maximum Supression (NMS) or to be exact, GreedyNMS, merges multiple detections into a single final detection. GreedyNMS selects the highest-scoring detection, thereafter discarding all detections whose overlap over a specified threshold $\theta$, and repeats this procedure on the remaining detections \cite{hosang_learning_2017}.

% TODO: Soft NMS
In addition to GreedyNMS, we apply SoftNMS with a sigma value of $\sigma = 0.1$ in our experiments. SoftNMS differs from GreedyNMS in that it decreases the confidence scores of overlapping detections based on their IoU with the highest-scoring box, rather than discarding them outright. This approach allows detections that partially overlap with a high-confidence box to still contribute to the final results, improving recall without substantially affecting precision \cite{bodla_soft-nms_2017}.


\subsubsection{\textit{Weighted Box Fusion} (WBF)}
% TODO: Tujuan, Mekanisme pengelompokan box berdasarkan IoU, Rumus penggabungan koordinat berbobot confidence, Parameter eksperimen: IoU threshold, skema bobot, Dampak terhadap stabilitas ensemble
Unlike Non-Maximum Suppression (NMS), which selects the single highest-confidence box and discards overlapping detections, WBF uses confidence scores of all proposed bounding boxes to construct the average boxes \cite{solovyev_weighted_2021}. In practice, the final bounding box is computed as a weighted average of the coordinates from all boxes in each overlapping group, with weights given by the confidence scores assigned by each model.

\subsubsection{\textit{Non-Maximum Weighted} (NMW)}
% TODO: – Tujuan, Mekanisme penurunan confidence berbasis overlap, Prosedur pemilihan box final, Parameter eksperimen: kurva penurunan bobot, IoU threshold, Dampak terhadap konsistensi deteksi

Finally, we also employed Non-Maximum Weighted (NMW) to combine overlapping bounding boxes. In this method, each box in a group contributes to the final prediction proportionally to a weight calculated as the product of its confidence score and its IoU with the most confident box. The final bounding box is then obtained as the weighted average of all boxes in the group \cite{zhou_cad_2017}. Unlike Weighted Boxes Fusion (WBF), which uses confidence scores from all models to compute the average, NMW incorporates both the confidence and spatial overlap (IoU) of boxes within a single prediction set, allowing it to exploit local agreement among overlapping detections.

\subsection{Evaluation metrics}
Finally, we evaluated the performance of each ensemble methods in the previous section using \textit{Average Precision (AP)} and \textit{Average Recall (AR)}.

\textbf{Average Precision (AP)} is calculated as the area under the Precision-Recall curve for each class, as defined in Equation~\eqref{eq:ap}:
\begin{equation}
\text{AP} = \int_0^1 \text{Precision}(r) \, dr
\label{eq:ap}
\end{equation}

\textbf{Average Recall (AR)} is computed as the average recall over multiple IoU thresholds for each class, as defined in Equation~\eqref{eq:ar}:
\begin{equation}
\text{AR} = \frac{1}{T} \sum_{t=1}^{T} \text{Recall}_{\text{IoU}=t}
\label{eq:ar}
\end{equation}

Higher AP and AR values indicate a model that is more accurate and better at detecting objects in the evaluation dataset.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

% \input{Sections/3_results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

% \input{Sections/4_discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used ``Conceptualization, X.X. and Y.Y.; methodology, X.X.; software, X.X.; validation, X.X., Y.Y. and Z.Z.; formal analysis, X.X.; investigation, X.X.; resources, X.X.; data curation, X.X.; writing---original draft preparation, X.X.; writing---review and editing, X.X.; visualization, X.X.; supervision, X.X.; project administration, X.X.; funding acquisition, Y.Y. All authors have read and agreed to the published version of the manuscript.'', please turn to the  \href{http://img.mdpi.org/data/contributor-role-instruction.pdf}{CRediT taxonomy} for the term explanation. Authorship must be limited to those who have contributed substantially to the work~reported.}

\funding{Please add: ``This research received no external funding'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.}

\institutionalreview{In this section, you should add the Institutional Review Board Statement and approval number, if relevant to your study. You might choose to exclude this statement if the study did not require ethical approval. Please note that the Editorial Office might ask you for further information. Please add “The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving humans. OR “The animal study protocol was approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving animals. OR “Ethical review and approval were waived for this study due to REASON (please provide a detailed justification).” OR “Not applicable” for studies not involving humans or animals.}

\informedconsent{Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.

    Written informed consent for publication must be obtained from participating patients who can be identified (including by the patients themselves). Please state ``Written informed consent has been obtained from the patient(s) to publish this paper'' if applicable.}

\dataavailability{We encourage all authors of articles published in MDPI journals to share their research data. In this section, please provide details regarding where data supporting reported results can be found, including links to publicly archived datasets analyzed or generated during the study. Where no new data were created, or where data is unavailable due to privacy or ethical restrictions, a statement is still required. Suggested Data Availability Statements are available in section ``MDPI Research Data Policies'' at \url{https://www.mdpi.com/ethics}.}

% Only for journal Drones
%\durcstatement{Current research is limited to the [please insert a specific academic field, e.g., XXX], which is beneficial [share benefits and/or primary use] and does not pose a threat to public health or national security. Authors acknowledge the dual-use potential of the research involving xxx and confirm that all necessary precautions have been taken to prevent potential misuse. As an ethical responsibility, authors strictly adhere to relevant national and international laws about DURC. Authors advocate for responsible deployment, ethical considerations, regulatory compliance, and transparent reporting to mitigate misuse risks and foster beneficial outcomes.}

% Only for journal Nursing Reports
%\publicinvolvement{Please describe how the public (patients, consumers, carers) were involved in the research. Consider reporting against the GRIPP2 (Guidance for Reporting Involvement of Patients and the Public) checklist. If the public were not involved in any aspect of the research add: ``No public involvement in any aspect of this research''.}
%
%% Only for journal Nursing Reports
%\guidelinesstandards{Please add a statement indicating which reporting guideline was used when drafting the report. For example, ``This manuscript was drafted against the XXX (the full name of reporting guidelines and citation) for XXX (type of research) research''. A complete list of reporting guidelines can be accessed via the equator network: \url{https://www.equator-network.org/}.}
%
%% Only for journal Nursing Reports
%\useofartificialintelligence{Please describe in detail any and all uses of artificial intelligence (AI) or AI-assisted tools used in the preparation of the manuscript. This may include, but is not limited to, language translation, language editing and grammar, or generating text. Alternatively, please state that “AI or AI-assisted tools were not used in drafting any aspect of this manuscript”.}

\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments). Where GenAI has been used for purposes such as generating text, data, or graphics, or for study design, data collection, analysis, or interpretation of data, please add “During the preparation of this manuscript/study, the author(s) used [tool name, version information] for the purposes of [description of use]. The authors have reviewed and edited the output and take full responsibility for the content of this publication.”}

\conflictsofinterest{Declare conflicts of interest or state ``The authors declare no conflicts of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript; or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results''.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
    The following abbreviations are used in this manuscript:
    \\

    \noindent
    \begin{tabular}{@{}ll}
        MDPI & Multidisciplinary Digital Publishing Institute \\
        DOAJ & Directory of open access journals              \\
        TLA  & Three letter acronym                           \\
        LD   & Linear dichroism
    \end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section[\appendixname~\thesection]{}
\subsection[\appendixname~\thesubsection]{}
The appendix is an optional section that can contain details and data supplemental to the main text---for example, explanations of experimental details that would disrupt the flow of the main text but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data are shown in the main text can be added here if brief, or as Supplementary Data. Mathematical proofs of results not central to the paper can be added as an appendix.

\begin{table}[H]
    \caption{This is a table caption.\label{tab5}}
    %\newcolumntype{C}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\textwidth}{CCC}
        \toprule
        \textbf{Title 1} & \textbf{Title 2} & \textbf{Title 3} \\
        \midrule
        Entry 1          & Data             & Data             \\
        Entry 2          & Data             & Data             \\
        \bottomrule
    \end{tabularx}
\end{table}

\section[\appendixname~\thesection]{}
All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
    %} % If the paper is ``preprints'', please uncomment this parenthesis.
    %\printendnotes[custom] % Un-comment to print a list of endnotes

    \reftitle{References}

    % Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
    % Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

    %=====================================
    % References, variant A: external bibliography
    %=====================================
    % \bibliography{your_external_BibTeX_file}

    %=====================================
    % References, variant B: internal bibliography
    %=====================================

    \bibliography{references}
    % If authors have biography, please use the format below
    %\section*{Short Biography of Authors}
    %\bio
    %{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
    %{\textbf{Firstname Lastname} Biography of first author}
    %
    %\bio
    %{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
    %{\textbf{Firstname Lastname} Biography of second author}

    % For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
    % To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
    % To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %% for journal Sci
    %\reviewreports{\\
    %Reviewer 1 comments and authors’ response\\
    %Reviewer 2 comments and authors’ response\\
    %Reviewer 3 comments and authors’ response
    %}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \PublishersNote{}
    %\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}

