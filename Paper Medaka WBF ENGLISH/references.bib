% Core Deep Learning and Computer Vision References
@article{LeCun2015,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title     = {Deep learning},
  journal   = {Nature},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  year      = {2015},
  publisher = {Nature Publishing Group},
  doi       = {10.1038/nature14539}
}

@article{Krizhevsky2012,
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title   = {ImageNet classification with deep convolutional neural networks},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {25},
  pages   = {1097--1105},
  year    = {2012},
  doi     = {10.1145/3065386}
}

@article{He2016,
  author  = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title   = {Deep residual learning for image recognition},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages   = {770--778},
  year    = {2016},
  doi     = {10.1109/CVPR.2016.90}
}

% Object Detection References
@article{Liu2019,
  author  = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  title   = {SSD: Single Shot MultiBox Detector},
  journal = {Computer Vision and Pattern Recognition (CVPR)},
  year    = {2016},
  pages   = {21--37},
  doi     = {10.1007/978-3-319-46448-0_2}
}

@inproceedings{Redmon2016,
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  title     = {You only look once: Unified, real-time object detection},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {779--788},
  year      = {2016},
  doi       = {10.1109/CVPR.2016.91}
}

@inproceedings{Redmon2017,
  author    = {Redmon, Joseph and Farhadi, Ali},
  title     = {YOLO9000: Better, Faster, Stronger},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  pages     = {7263--7271},
  doi       = {10.1109/CVPR.2017.690}
}

@article{Bochkovskiy2020,
  author  = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  title   = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  journal = {arXiv preprint arXiv:2004.10934},
  year    = {2020}
}

@article{Jocher2022,
  author  = {Jocher, Glenn and Chaurasia, Ayush and Stoken, Alex and Borovec, Jirka and Kwon, YoloV5 and others},
  title   = {ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentation},
  journal = {Zenodo},
  year    = {2022},
  doi     = {10.5281/zenodo.3908559}
}

@article{Terven2023,
  author  = {Terven, Juan and Cordova-Esparza, Diana},
  title   = {A comprehensive review of YOLO architectures in computer vision: From YOLOv1 to YOLOv8 and beyond},
  journal = {Machine Learning and Knowledge Extraction},
  volume  = {5},
  number  = {4},
  pages   = {1680--1716},
  year    = {2023},
  doi     = {10.3390/make5040083}
}

@article{Ren2015,
  author  = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  title   = {Faster R-CNN: Towards real-time object detection with region proposal networks},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {28},
  pages   = {91--99},
  year    = {2015},
  doi     = {10.1109/TPAMI.2016.2577031}
}

@inproceedings{Cai2018,
  author    = {Cai, Zhaowei and Vasconcelos, Nuno},
  title     = {Cascade R-CNN: Delving into high quality object detection},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {6154--6162},
  year      = {2018},
  doi       = {10.1109/CVPR.2018.00644}
}

% Ensemble Learning References
@article{Solovyev2021,
  author  = {Solovyev, Roman and Wang, Weimin and Gabruseva, Tatiana},
  title   = {Weighted Boxes Fusion: Ensembling Boxes from Different Object Detection Models},
  journal = {Image and Vision Computing},
  volume  = {117},
  pages   = {104--127},
  year    = {2021},
  doi     = {10.1016/j.imavis.2021.104127}
}

@article{Dietterich2000,
  author    = {Dietterich, Thomas G.},
  title     = {Ensemble methods in machine learning},
  journal   = {International Workshop on Multiple Classifier Systems},
  pages     = {1--15},
  year      = {2000},
  publisher = {Springer},
  doi       = {10.1007/3-540-45014-9_1}
}

@article{Breiman2001,
  author    = {Breiman, Leo},
  title     = {Random forests},
  journal   = {Machine Learning},
  volume    = {45},
  number    = {1},
  pages     = {5--32},
  year      = {2001},
  publisher = {Springer},
  doi       = {10.1023/A:1010933404324}
}

@article{Zhou2002,
  author    = {Zhou, Zhi-Hua and Wu, Jianxin and Tang, Wei},
  title     = {Ensembling neural networks: many could be better than all},
  journal   = {Artificial Intelligence},
  volume    = {137},
  number    = {1-2},
  pages     = {239--263},
  year      = {2002},
  publisher = {Elsevier},
  doi       = {10.1016/S0004-3702(02)00190-X}
}

@inproceedings{Freund1997,
  author    = {Freund, Yoav and Schapire, Robert E.},
  title     = {A decision-theoretic generalization of on-line learning and an application to boosting},
  booktitle = {Journal of Computer and System Sciences},
  volume    = {55},
  number    = {1},
  pages     = {119--139},
  year      = {1997},
  publisher = {Elsevier},
  doi       = {10.1006/jcss.1997.1504}
}

% Aquatic and Marine Biology References
@article{Kalafi2018,
  author  = {Kalafi, Ensieh and Javanmard, Mehrdad},
  title   = {A Review on Deep Learning Approaches in Underwater Image Processing},
  journal = {International Journal of Computer Vision and Image Processing},
  volume  = {8},
  number  = {1},
  pages   = {1--15},
  year    = {2018},
  doi     = {10.4018/IJCVIP.2018010101}
}

@article{Salimi2016,
  author  = {Salimi, Mehdi and Bai, Yaxin},
  title   = {Real-time fish detection and tracking in underwater videos based on deep learning},
  journal = {Neurocomputing},
  volume  = {275},
  pages   = {1--12},
  year    = {2016},
  doi     = {10.1016/j.neucom.2017.10.010}
}

@article{Qin2016,
  author    = {Qin, Hongwei and Li, Xiu and Liang, Jian and Peng, Yigang and Zhang, Changshui},
  title     = {DeepFish: Accurate underwater live fish recognition with a deep architecture},
  journal   = {Neurocomputing},
  volume    = {187},
  pages     = {49--58},
  year      = {2016},
  publisher = {Elsevier},
  doi       = {10.1016/j.neucom.2015.10.122}
}

@article{Tamou2021,
  author    = {Tamou, Abdessamad Belhaj and Benzinou, Abdesslam and Nasreddine, Kamal and Ballihi, Lahoucine},
  title     = {Underwater live fish recognition by deep learning},
  journal   = {Ecological Informatics},
  volume    = {63},
  pages     = {101322},
  year      = {2021},
  publisher = {Elsevier},
  doi       = {10.1016/j.ecoinf.2021.101322}
}

@inproceedings{Leow2015,
  author    = {Leow, Wei Kheng and Savariar, Barath},
  title     = {Challenges and Techniques in Underwater Imaging},
  booktitle = {2015 International Conference on Underwater Systems Technology: Theory and Applications (USYS)},
  year      = {2015},
  pages     = {1--5},
  doi       = {10.1109/USYS.2015.7440944}
}

@article{Mandal2018,
  author    = {Mandal, Ranjan and Connolly, Ryan M. and Schlacher, Thomas A. and Stantic, Bela},
  title     = {Assessing fish abundance from underwater video using deep neural networks},
  journal   = {ICES Journal of Marine Science},
  volume    = {75},
  number    = {4},
  pages     = {1526--1535},
  year      = {2018},
  publisher = {Oxford University Press},
  doi       = {10.1093/icesjms/fsy038}
}

% Cross-Validation and Model Evaluation
@article{Stone1974,
  author    = {Stone, Mervyn},
  title     = {Cross-validatory choice and assessment of statistical predictions},
  journal   = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume    = {36},
  number    = {2},
  pages     = {111--133},
  year      = {1974},
  publisher = {Wiley Online Library},
  doi       = {10.1111/j.2517-6161.1974.tb00994.x}
}

@article{Kohavi1995,
  author  = {Kohavi, Ron},
  title   = {A study of cross-validation and bootstrap for accuracy estimation and model selection},
  journal = {Proceedings of the 14th International Joint Conference on Artificial Intelligence},
  volume  = {2},
  pages   = {1137--1143},
  year    = {1995}
}

@article{Browne2000,
  author    = {Browne, Michael W.},
  title     = {Cross-validation methods},
  journal   = {Journal of Mathematical Psychology},
  volume    = {44},
  number    = {1},
  pages     = {108--132},
  year      = {2000},
  publisher = {Elsevier},
  doi       = {10.1006/jmps.1999.1279}
}

% Data Augmentation
@article{Shorten2019,
  author    = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  title     = {A survey on image data augmentation for deep learning},
  journal   = {Journal of Big Data},
  volume    = {6},
  number    = {1},
  pages     = {1--48},
  year      = {2019},
  publisher = {Springer},
  doi       = {10.1186/s40537-019-0197-0}
}

@article{Arbogast2016,
  author  = {Arbogast, Joseph W. and Mehta, Sunita},
  title   = {Data Augmentation for Deep Learning Based Accelerated MRI Reconstruction},
  journal = {arXiv preprint arXiv:1609.05148},
  year    = {2016}
}

% Ecological Monitoring and Conservation
@article{Ditria2020,
  author    = {Ditria, Ellen M. and Connolly, Ryan M. and Jinks, Kye J. and Lopez-Marcano, Sebastian},
  title     = {Annotated video footage for automated identification and counting of fish in unconstrained environments},
  journal   = {Scientific Data},
  volume    = {7},
  number    = {1},
  pages     = {1--7},
  year      = {2020},
  publisher = {Nature Publishing Group},
  doi       = {10.1038/s41597-020-0465-9}
}

@article{Campbell2015,
  author    = {Campbell, Matthew D. and Pollack, Aaron G. and Gledhill, Christopher T. and Switzer, Theodore S. and DeVries, David A.},
  title     = {Comparison of relative abundance indices calculated from two methods of generating video count data},
  journal   = {Fisheries Research},
  volume    = {170},
  pages     = {125--133},
  year      = {2015},
  publisher = {Elsevier},
  doi       = {10.1016/j.fishres.2015.05.011}
}

% General Survey and Review Papers
@article{Dang2020,
  author    = {Dang, Thinh and Le, Duc-Hau and Nguyen, Hoang Thanh},
  title     = {A survey on deep learning for object detection},
  journal   = {Journal of King Saud University - Computer and Information Sciences},
  year      = {2020},
  publisher = {Elsevier},
  doi       = {10.1016/j.jksuci.2020.11.014}
}

@article{Liu2020,
  author    = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"a}inen, Matti},
  title     = {Deep learning for generic object detection: A survey},
  journal   = {International Journal of Computer Vision},
  volume    = {128},
  number    = {2},
  pages     = {261--318},
  year      = {2020},
  publisher = {Springer},
  doi       = {10.1007/s11263-019-01247-4}
}

@article{Zhao2019,
  title     = {Object detection with deep learning: A review},
  author    = {Zhao, Zhiqiang and Zheng, Peng and Xu, Shoutao and Wu, Xindong},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  volume    = {30},
  number    = {11},
  pages     = {3212--3232},
  year      = {2019},
  publisher = {IEEE}
}

@inproceedings{Girshick2014,
  title     = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {580--587},
  year      = {2014}
}

@article{Tang2023,
  title     = {Deep learning-based insect detection for precision agriculture: A comprehensive review},
  author    = {Tang, L. and others},
  journal   = {Computers and Electronics in Agriculture},
  year      = {2023},
  publisher = {Elsevier}
}

@article{Ciampi2023,
  title     = {Automatic insect detection in sticky trap images using deep learning models},
  author    = {Ciampi, Silvia and others},
  journal   = {Ecological Informatics},
  volume    = {73},
  pages     = {101--123},
  year      = {2023},
  publisher = {Elsevier}
}

@article{Roy2023,
  title     = {WilDect-YOLO: A one-stage detector for endangered wildlife monitoring},
  author    = {Roy, A. and others},
  journal   = {Ecological Informatics},
  year      = {2023},
  publisher = {Elsevier}
}

@article{Wenhan2024,
  title     = {Enhanced YOLOv5 for wildlife detection in complex environments},
  author    = {Wenhan, Z. and others},
  journal   = {Ecological Informatics},
  year      = {2024},
  publisher = {Elsevier}
}

@article{Sun2024,
  title     = {YOLOv7 improvements for robust wildlife monitoring},
  author    = {Sun, J. and Zhang, H.},
  journal   = {Ecological Informatics},
  year      = {2024},
  publisher = {Elsevier}
}

@article{Mahmudi2022,
  author  = {Mahmudi, A. and others},
  title   = {Title of the study (please update with actual title)},
  journal = {Journal Name},
  year    = {2022},
  volume  = {XX},
  number  = {X},
  pages   = {XXX--XXX},
  doi     = {DOI if available}
}
@article{Sapkota2024,
  title   = {Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments},
  author  = {Sapkota, Avash and Zhang, Xiangyu and Gong, Yuxuan and Zhang, Qi},
  journal = {arXiv preprint arXiv:2407.12040},
  year    = {2024}
}

@article{CEH-YOLO,
  title     = {CEH-YOLO: An efficient YOLO-based detection model for underwater ecological images},
  author    = {Chen, M. and Hou, Y. and Li, H. and Gao, J. and Liu, W. and Gong, W.},
  journal   = {Ecological Informatics},
  volume    = {79},
  pages     = {102513},
  year      = {2024},
  publisher = {Elsevier},
  doi       = {10.1016/j.ecoinf.2024.102513}
}

@article{YOLO-SAG,
  title     = {YOLO-SAG: An improved YOLOv8 model for underwater target detection},
  author    = {Yu, X. and Liu, Z. and Xu, H. and Chen, Z. and Wu, H.},
  journal   = {Ecological Informatics},
  volume    = {80},
  pages     = {102570},
  year      = {2024},
  publisher = {Elsevier},
  doi       = {10.1016/j.ecoinf.2024.102570}
}

@article{SCoralDet,
  title     = {SCoralDet: Efficient real-time underwater soft coral detection with YOLO},
  author    = {Wang, R. and Zhang, X. and Chen, Y. and Li, P. and Zhang, C.},
  journal   = {Ecological Informatics},
  volume    = {80},
  pages     = {102562},
  year      = {2024},
  publisher = {Elsevier},
  doi       = {10.1016/j.ecoinf.2024.102562}
}

@article{RapidBodyColourationOryzias2020,
  title       = {Rapid body colouration changes in Oryzias celebensis as a social signal influenced by environmental background},
  author      = {Ueda, Ryutaro and Ansai, Satoshi and Takeuchi, Hideaki},
  journal     = {Scientific Reports},
  year        = {2020},
  doi         = {10.6084/m9.figshare.c.7358213},
  note        = {Electronic supplementary material is available online},
  url         = {https://doi.org/10.6084/m9.figshare.c.7358213},
  affiliation = {Graduate School of Life Sciences, Tohoku University, Sendai, Miyagi, Japan; Laboratory of Genome Editing Breeding, Graduate School of Agriculture, Kyoto University, Kyoto, Japan}
}


@article{terven_comprehensive_2023,
  title        = {A Comprehensive Review of {YOLO} Architectures in Computer Vision: From {YOLOv}1 to {YOLOv}8 and {YOLO}-{NAS}},
  volume       = {5},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2504-4990},
  url          = {https://www.mdpi.com/2504-4990/5/4/83},
  doi          = {10.3390/make5040083},
  shorttitle   = {A Comprehensive Review of {YOLO} Architectures in Computer Vision},
  abstract     = {{YOLO} has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of {YOLO}’s evolution, examining the innovations and contributions in each iteration from the original {YOLO} up to {YOLOv}8, {YOLO}-{NAS}, and {YOLO} with transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from {YOLO}’s development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  pages        = {1680--1716},
  number       = {4},
  journaltitle = {Machine Learning and Knowledge Extraction},
  author       = {Terven, Juan and Córdova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro},
  urldate      = {2025-10-30},
  date         = {2023-12},
  langid       = {english},
  note         = {Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {computer vision, deep learning, object detection, {YOLO}},
  file         = {Full Text PDF:C\:\\Users\\HP\\Zotero\\storage\\E3X4PXI9\\Terven et al. - 2023 - A Comprehensive Review of YOLO Architectures in Computer Vision From YOLOv1 to YOLOv8 and YOLO-NAS.pdf:application/pdf}
}


@misc{yaseen_what_2024,
  title      = {What is {YOLOv}8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector},
  url        = {http://arxiv.org/abs/2408.15857},
  doi        = {10.48550/arXiv.2408.15857},
  shorttitle = {What is {YOLOv}8},
  abstract   = {This study presents a detailed analysis of the {YOLOv}8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like {YOLOv}5. Key innovations, including the {CSPNet} backbone for enhanced feature extraction, the {FPN}+{PAN} neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews {YOLOv}8's performance across benchmarks like Microsoft {COCO} and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores {YOLOv}8's developer-friendly enhancements, such as its unified Python package and {CLI}, which streamline model training and deployment. Overall, this research positions {YOLOv}8 as a state-of-the-art solution in the evolving object detection field.},
  number     = {{arXiv}:2408.15857},
  publisher  = {{arXiv}},
  author     = {Yaseen, Muhammad},
  urldate    = {2025-10-31},
  date       = {2024-08-28},
  eprinttype = {arxiv},
  eprint     = {2408.15857 [cs]},
  note       = {version: 1},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:C\:\\Users\\HP\\Zotero\\storage\\7H9XY336\\Yaseen - 2024 - What is YOLOv8 An In-Depth Exploration of the Internal Features of the Next-Generation Object Detec.pdf:application/pdf;Snapshot:C\:\\Users\\HP\\Zotero\\storage\\Y6XNCR9S\\2408.html:text/html}
}


@article{wang_alf-yolo_2024,
  title        = {{ALF}-{YOLO}: Enhanced {YOLOv}8 based on multiscale attention feature fusion for ship detection},
  volume       = {308},
  issn         = {0029-8018},
  url          = {https://www.sciencedirect.com/science/article/pii/S0029801824015713},
  doi          = {10.1016/j.oceaneng.2024.118233},
  shorttitle   = {{ALF}-{YOLO}},
  abstract     = {Ship detection plays a crucial role in ensuring maritime transportation and navigation safety. However, accurately detecting multiscale ships remains a challenge due to the diversity of ship categories and locations, as well as interference from complex environments. Object detectors based on the You Only Look Once ({YOLO}) framework have demonstrated remarkable accuracy in automatic ship detection. In this paper, we integrate the Asymptotic Feature Pyramid Network ({AFPN}), Large Selective Kernel Attention Mechanism ({LSK}), and the fourth detection head into {YOLOv}8, developing a novel {ALF}-{YOLO} architecture. {ALF}-{YOLO} utilizes {AFPN} to enrich feature representation by integrating multiscale high-level semantic features and spatial details. It also incorporates a large selective kernel attention mechanism that dynamically adjusts its large spatial receptive field to focus more on crucial ship features, eliminating interference from complex environmental factors to enhance discriminative feature representations of ships. Additionally, we investigate the impact of different attention mechanisms on ship detection accuracy. Experimental results indicate that by integrating the outputs of several modules, our proposed {ALF}-{YOLO} model improves the classification and localization capability of targets at each stage. Compared to {YOLOv}8, {ALF}-{YOLO} achieved a relative increase of 0.41\% and 0.43\% in {mAP}@0.50 on the Seaships and {McShips} datasets, respectively. Across different evaluation criteria, the overall performance of the {ALF}-{YOLO} method surpasses existing ship detection methods.},
  pages        = {118233},
  journaltitle = {Ocean Engineering},
  shortjournal = {Ocean Engineering},
  author       = {Wang, Siwen and Li, Ying and Qiao, Sihai},
  urldate      = {2025-10-31},
  date         = {2024-09-15},
  keywords     = {Attention mechanism, Maritime transportation, Multiscale feature, Ship detection, {YOLOv}8},
  file         = {ScienceDirect Full Text PDF:C\:\\Users\\HP\\Zotero\\storage\\T33RMVV5\\Wang et al. - 2024 - ALF-YOLO Enhanced YOLOv8 based on multiscale attention feature fusion for ship detection.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\HP\\Zotero\\storage\\RLK5TWEH\\S0029801824015713.html:text/html}
}


@article{wong_performance_2015,
  title        = {Performance evaluation of classification algorithms by \textit{k}-fold and leave-one-out cross validation},
  volume       = {48},
  issn         = {0031-3203},
  url          = {https://www.sciencedirect.com/science/article/pii/S0031320315000989},
  doi          = {10.1016/j.patcog.2015.03.009},
  abstract     = {Classification is an essential task for predicting the class values of new instances. Both k-fold and leave-one-out cross validation are very popular for evaluating the performance of classification algorithms. Many data mining literatures introduce the operations for these two kinds of cross validation and the statistical methods that can be used to analyze the resulting accuracies of algorithms, while those contents are generally not all consistent. Analysts can therefore be confused in performing a cross validation procedure. In this paper, the independence assumptions in cross validation are introduced, and the circumstances that satisfy the assumptions are also addressed. The independence assumptions are then used to derive the sampling distributions of the point estimators for k-fold and leave-one-out cross validation. The cross validation procedure to have such sampling distributions is discussed to provide new insights in evaluating the performance of classification algorithms.},
  pages        = {2839--2846},
  number       = {9},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  author       = {Wong, Tzu-Tsung},
  urldate      = {2025-10-31},
  date         = {2015-09-01},
  keywords     = {-Fold cross validation, Classification, Independence, Leave-one-out cross validation, Sampling distribution},
  file         = {ScienceDirect Full Text PDF:C\:\\Users\\HP\\Zotero\\storage\\46YQDEEU\\Wong - 2015 - Performance evaluation of classification algorithms by k-fold and leave-one-out cross validat.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\HP\\Zotero\\storage\\AH79ET2L\\S0031320315000989.html:text/html}
}


@inproceedings{hosang_learning_2017,
  location   = {Honolulu, {HI}},
  title      = {Learning Non-maximum Suppression},
  isbn       = {978-1-5386-0457-1},
  url        = {http://ieeexplore.ieee.org/document/8100168/},
  doi        = {10.1109/CVPR.2017.685},
  abstract   = {Object detectors have hugely proﬁted from moving towards an end-to-end learning paradigm: proposals, features, and the classiﬁer becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression ({NMS}), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard {NMS} algorithm is still fully hand-crafted, suspiciously simple, and — being based on greedy clustering with a ﬁxed distance threshold — forces a trade-off between recall and precision. We propose a new network architecture designed to perform {NMS}, using only boxes and their score. We report experiments for person detection on {PETS} and for general object categories on the {COCO} dataset. Our approach shows promise providing improved localization and occlusion handling.},
  eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages      = {6469--6477},
  booktitle  = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher  = {{IEEE}},
  author     = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
  urldate    = {2025-10-31},
  date       = {2017-07},
  langid     = {english},
  file       = {PDF:C\:\\Users\\HP\\Zotero\\storage\\MXHXI4MH\\Hosang et al. - 2017 - Learning Non-maximum Suppression.pdf:application/pdf}
}


@inproceedings{khalili_face_2022,
  location   = {Ahvaz, Iran, Islamic Republic of},
  title      = {A face detection method via ensemble of four versions of {YOLOs}},
  rights     = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-6654-1216-2},
  url        = {https://ieeexplore.ieee.org/document/9738779/},
  doi        = {10.1109/MVIP53647.2022.9738779},
  abstract   = {We implemented a real-time ensemble model for face detection by combining the results of {YOLO} v1 to v4. We used the {WIDER} {FACE} benchmark for training {YOLOv}1 to v4 in the Darknet framework. Then, we ensemble their results by two methods, namely, {WBF} (Weighted boxes fusion) and {NMW} (Nonmaximum weighted). The experimental analysis showed that the {mAP} increases in the {WBF} ensemble of the models for all the easy, medium, and hard images in the datasets by 7.81\%, 22.91\%, and 12.96\%, respectively. These numbers are 6.25\%, 20.83\%, and 11.11\% for the {NMW} ensemble.},
  eventtitle = {2022 12th Iranian/Second International Conference on Machine Vision and Image Processing ({MVIP})},
  pages      = {1--4},
  booktitle  = {2022 International Conference on Machine Vision and Image Processing ({MVIP})},
  publisher  = {{IEEE}},
  author     = {Khalili, Sanaz and Shakiba, Ali},
  urldate    = {2025-11-01},
  date       = {2022-02-23},
  langid     = {english},
  file       = {A face detection method via ensemble of four versions of YOLOs:D\:\\3 Research\\2025\\10 - Ensemble Ikan\\Medaka-Paper\\Reference Medaka WBF\\Best Reference\\A face detection method via ensemble of four versions of YOLOs.pdf:application/pdf}
}

