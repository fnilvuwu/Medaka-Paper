%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi} 

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, analytica, analytics, anatomia, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, future, futureinternet, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, grasses, gucdd, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijpb, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidneydial, kinasesphosphatases, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing,\gdef\@continuouspages{yes}} nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, %%nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, %oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology,\gdef\@ISSN{2813-0618}\gdef\@continuous pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2023}
\copyrightyear{2023}
%\externaleditor{Academic Editor: Firstname Lastname}
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For corrected papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{YOLO-Medaka: An Automatic Object Detection Models for Endangered Medaka Fishes in Indonesia }

% MDPI internal command: Title for citation in the left column
\TitleCitation{Title}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0000-0000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{
Armin Lawi $^{1, 2, 3,\orcidA{}}$, 
Irma Andriani $^4$, 
Andi Iqbal Burhanuddin $^5$, and 
Mario Köppen $^6$
}

%\Author{Nurdiansyah Sirimorok * \orcidA{}, Rio Mukhtarom Paweroi \orcidB{}, Andi Arniaty Arsyad and Mario Köppen}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Armin Lawi, Irma Andriani, Andi Iqbal Burhanuddin and Mario Koeppen}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Lawi, A. ; Adriani, I. ; Burhanuddin, A.I. and Köppen, M}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Information Systems Study Origram, Faculty of Mathematics and Natural Sciences, Hasanuddin University, Indonesia \\
$^{2}$ \quad Data Science and Artificial Intelligence Research Group, Hasanuddin University, Indonesia \\
$^{3}$ \quad B.J. Habibie Institute of Technology, Parepare, Indonesia \\
$^{4}$ \quad Department of Biology, Faculty of Mathematics and Natural Sciences, Hasanuddin University, Indonesia \\
$^{5}$ \quad Department of Fishery, Faculty of Fishery and Marine Sciences, Hasanuddin University, Indonesia \\
$^{6}$ \quad Department of Creative Informatics, Faculty of Computer Science and Systems Engineering, Kyushu Institute of Technology, Japan}

% Contact information of the corresponding author
\corres{Correspondence : armin@unhas.ac.id)}

% Current address and/or shared authorship
%R\firstnote{Armin Lawi : Department of Information Systems, Faculty of Mathematics and Natural Sciences, Hasanuddin University, Indonesia.} 
%\secondnote{Irma Andriani, Andi Iqbal Burhanuddin, and Mario Köppen.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{\textit{Oryzias Celebensis} and \textit{Oryzias Javanicus} are two species of endemic fish that play a crucial role in the aquatic ecosystem in Indonesia, especially in Java and Sulawesi regions. But unfortunately, both species are threatened with extinction and have been officially declared by the International Union for Conservation of Nature and Natural Resources (IUCN). To address this problem, Artificial Intelligence innovation was built with a Deep Learning approach to conduct classification and identification of the fish species quickly and efficiently. In this study, we developed the MobileNetV2 architecture model using the transfer learning approach and dataset padding and then embedded it into the YOLOv8 architecture on the backbone as the main architecture in this system to classify \textit{Oryzias Celebensis} and \textit{Oryzias Javanicus}. The P-MobileNetV2 model has better performance evaluation results than other models, where sensitivity = $98.46\%$, Precision = $98.46\%$, F1 Score = $98.46\%$ and Accuracy = $98.7\%$. This advancement enables rapid, accurate conservation efforts for these critically endangered fish}

% Keywords
\keyword{Artificial Intelligence; Deep Learning; Tranfer Learning; MobileNetV2; Padding Dataset.} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%

%\noindent This is an obligatory section in “Advances in Respiratory Medicine”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}

%The template details the sections that can be used in a manuscript. Note that the order and names of article sections may differ from the requirements of the journal (e.g., the positioning of the Materials and Methods section). Please check the instructions on the authors' page of the journal to verify the correct order and names. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX-related questions please contact latex@mdpi.com.%\endnote{This is an endnote.} % To use endnotes, please un-comment \printendnotes below (before References). Only journal Laws uses \footnote.

% The order of the section titles is different for some journals. Please refer to the "Instructions for Authors” on the journal homepage.

\section{Introduction}

\textit{Oryzias celebensis} and \textit{Oryzias javanicus} are two endemic fish species that play a crucial role in the aquatic ecosystems of Indonesia, particularly in the Sulawesi and Java regions \cite{parenti2008phylogenetic,herder2022more}. Both species play an important role in maintaining biodiversity and ecosystem balance \cite{nurdin2023perubahan}. Identifying rare fish species is a crucial first step in conservation efforts. By recognizing morphological characteristics, genetic analysis, and behavioral understanding, we can gain better insight into these species. Identification also helps us take more effective action to protect habitats and prevent extinction. 

The genus \textit{Oryzias} belongs to the family \textit{Adrianichthyidae}. These fish are widely distributed in South Asia, East Asia, and Southeast Asia. Their natural habitat includes rice fields, ponds, ditches, and lakes \cite{kottelat2013fishes,magtoon2009revised}. \textit{O. celebensis} and \textit{O. javanicus} are two species particularly threatened with extinction in Indonesia.  Celebes Medaka (\textit{O. celebensis}) is an endemic species on Sulawesi Island \cite{parenti2011endemism}. Unfortunately, the Medaka fish is an endemic species threatened with extinction and has been officially declared as such by the International Union for Conservation of Nature (IUCN). Conservation efforts are critically important to prevent the extinction of these species. Oryzias javanicus: This fish is also known as the Java Medaka. \cite{lamba2023habitat}

Unfortunately, we realize that the populations of Oryzias celebensis and Oryzias javanicus are declining, pushing these two species to the brink of extinction. This situation is even more concerning because both are categorized as rare fish species endemic to Indonesia, specifically found in Sulawesi and Java. Threats to their natural habitats, climate change, and human activities that damage aquatic environments are further worsening their condition. \cite{MOKODONGAN2015150}In efforts to protect and conserve these rare species, researchers often face ethical dilemmas during the identification process. Conventional methods commonly used, such as examining dead fish or taking fish out of water, which may harm them, are no longer acceptable in the context of environmental sustainability and conservation \cite{voss2014assessing}.

To address this challenge, innovations in artificial intelligence, particularly deep learning technology, offer a promising solution. Using advanced computational capabilities, researchers can use deep learning models to classify and identify fish species, including Oryzias celebensis and Oryzias javanicus, quickly and efficiently. This approach is not only more environmentally friendly, but also enables more effective monitoring of these rare fish populations, supporting conservation efforts to ensure their survival in the future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works and Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work, MobileNetV2 is utilized for image-based object classification. This model is an enhanced version of the previous MobileNetV1 architecture. MobileNetV2 is a Convolutional Neural Network (CNN) specifically designed for resource-constrained devices, such as smartphones, IoT devices, and embedded systems. Developed by Google AI and published in 2018, this model offers several advantages highlighted in various studies, including its lightweight architecture, competitive performance, and compatibility with several deep learning frameworks~\cite{8578572,akay2021deep}. The model combines an inverse residual block with a linear bottleneck, which improves efficiency while maintaining accuracy. These efficiency advantages make MobileNetV2 suitable for real-time applications on low-power devices. General structure of the MobileNetV2 model. 

%as illustrated in the framework, can be seen in Figure \ref{fig: MobileNetV2 Architecture}.
%\begin{figure}[h]
%\includegraphics[width=1\textwidth]
%{Definitions/MobileNetV2.png}
%\caption{General MobileNetV2 Architecture.
%\label{fig: MobileNetV2 Architecture}}
%\end{figure}

In study \cite{golcuk2023classification}, images of Cicer arietinum (chickpea) varieties were input into two architectures. The first used transfer learning with fine-tuning on a pre-trained MobileNetV2 CNN model for classification. The second was a hybrid architecture incorporating Long Short-Term Memory (LSTM) layers to account for temporal features. The results showed $92.3\%$ accuracy for the first model and $92.97\%$ for the hybrid, demonstrating a high success in classifying chickpea images. Additionally, authors \cite{sen2021face} applied a deep learning approach using MobileNetV2 within a PyTorch and OpenCV Python framework for mask detection during COVID-19. Their model efficiently identified proper mask usage, highlighting MobileNetV2's adaptability for diverse image-based tasks, from agricultural classification to public health applications. These studies underscore its effectiveness in achieving high accuracy while maintaining computational efficiency for real-world deployments.

In another previous study\cite{gulzar2023fruit}, the authors introduced customized heads comprising five different layers into the MobileNetV2 architecture. The model’s original classification layers were replaced with these customized heads, resulting in a modified version called TL-MobileNetV2. This adaptation achieved an accuracy of $99\%$ $—3\%$ higher than standard MobileNetV2, while maintaining a minimal error rate of just $1\%$. When compared to AlexNet, VGG16, InceptionV3, and ResNet, TL-MobileNetV2 demonstrated superior performance. Meanwhile, authors \cite{indraswari2022melanoma} implemented MobileNetV2 for melanoma cancer classification, reporting an accuracy of up to $85\%$ on the ISIC-Archive dataset, outperforming models like ResNet50V2, InceptionV3, and InceptionResNetV2. Another study \cite{shahoveisi2023application} evaluated four convolutional neural networks—Xception, ResNet50, EfficientNetB4, and MobileNet—for detecting rust disease in three commercially important field crops. Results showed EfficientNetB4 as the most accurate (average accuracy = $94.29\%$), followed by ResNet50 ($93.52\%$). Though MobileNetV2 trailed slightly in this comparison, its computational efficiency makes it viable for resource-constrained applications, reinforcing its versatility across medical and agricultural domains. Further research \cite{hamid2022smart} uses MobileNetV2, a deep learning convolutional neural network (DCNN) for seed classification. A total of 14 different classes of seeds were used for the experimentation. The results indicate accuracies of $98\%$ and $95\%$ on training and test sets, respectively.

\begin{table}[H]
\caption{Gap description in building research motivation.\label{tab: tablegap}}
	\begin{adjustwidth}{-\extralength}{0cm}
		\newcolumntype{C}{>{\centering\arraybackslash}X}
		\begin{tabularx}{\fulllength}{m{5mm} m{8.5cm} m{8.5cm}}
			\toprule
            \textbf{No.}& \textbf{Previous Works}& \textbf{Current Works}\\
			\midrule
            \multirow[m]{0.5}{*}{1} & 
                The hybrid architecture incorporates Long Short-Term Memory (LSTM) layers, which also account for temporal data features in classification. \cite{golcuk2023classification} & 
                The image data features four different background colors: red, black, blue, and green, with overhead lighting to ensure the fish are clearly visible.\\
                \midrule
            \multirow[m]{0.5}{*}{2}  & 
                Implementation of a deep learning approach using the MobileNetV2 framework, integrated with PyTorch and OpenCV in Python, for mask detection during COVID-19. \cite{sen2021face}	& 
                We developed this application using Jupyter Notebook, TensorFlow, Roboflow, and OpenCV for the identification of \textit{Oryzias celebensis} and \textit{Oryzias javanicus}.\\
                \midrule
            \multirow[m]{0.5}{*}{3} & 
                Implementing MobileNetV2 for Melanoma Cancer Classification. MobileNetV2 demonstrates higher accuracy compared to ResNet50V2, InceptionV3, and InceptionResNetV2. \cite{indraswari2022melanoma} & Development of a MobileNetV2 architecture using a transfer learning approach with dataset padding and additional layers for the Classification Layer. This layer consists of 5 components: a Flatten layer, two Dense layers with ReLU activation functions.\\
                \midrule
            \multirow[m]{0.5}{*}{4} & 
                A deep learning convolutional neural network (DCNN) which utilising MobileNetV2 for seed classification. \cite{hamid2022smart} & 
                We developed modified versions: P-MobileNetV2 (from MobileNetV2) and P-VGG16 (from VGG16) to evaluate the effects of our architectural changes.\\
                \bottomrule
		\end{tabularx}
	\end{adjustwidth}
\end{table}

Many researchers have proposed various combinations of Machine Learning and Deep Learning (ML/DL) algorithms to solve the problem of object detection, classification, and identification in digital images. In this paper, the YOLO-Medaka algorithm is introduced to detect and classify digital objects by combining three methods of YOLOv8, MobileNetV2 and ANN-MLP architecture. YOLOv8 is used for object detection because it has a single-stage detection concept, which means detecting objects at one time and has excellent accuracy. As seen in several experiments. for example, after modification, namely YOLOv8-CAB, it succeeds in detecting some objects that are missing detection by YOLOv8 before, also achieves higher accuracy and much better detection confidence. \cite{talib2024yolov8}. Another experiment also conducts object detection, such as analysing vehicle detection under various image conditions. This study utilises the YOLOv8 method to process the images with output the bounding boxes and classes of the detected objects. Additionally, data augmentation is applied to improve the model's ability to recognise vehicles from different directions and viewpoints \cite{panja2024yolov8}.

MobileNetV2 is deployed for mobile devices such as smartphones and tablets, which focus on computational efficiency and smaller model size, since MobileNetV2 is designed for mobile devices and embedded vision applications. As seen in the fruit classification experiment that compared the performance between MobileNetV2 and Inceptionv3, the results show that MobileNetV2 has a better accuracy rate, which means better performance than Inceptionv3. In an experiment with a different case, comparing the performance of MobileNetV2 with DenseNet121 for the classification of coral reefs, the results show that MobileNetV2 is more optimal for devices with limited computing power and is lighter and faster \cite{karnadi2024klasifikasi,utomo2025perbandingan}. In addition, the MobileNetV2 architecture has proven effective in increasing computational efficiency without sacrificing accuracy, which is an important aspect for implementing this model on mobile devices. High computational efficiency is possible for implementation in the real world, where computing resources may be limited \cite{maulana2024deteksi}.

ANN-MLP is tasked with processing the data conducted by the input, hidden and output layers they have, then producing the classification of $\textit{Oryzias celebensis}$ and $\textit{Oryzias javanicus}$. ANN-MLP is one of the algorithms that is widely used because of its ease of implementation in web applications (client/server or full stack programming). Some of its implementations include the classification of marine fish even though they are covered by seaweed or coral, the classification of types of diseases in aquatic plants, and the classification of certain problems or certain cases, both inherent in objects and their environment. This paper specifically focuses on the automatic detection and classification of two small, rare and endangered freshwater fish species in Indonesia, namely, $\textit{Oryzias celebensis}$ and $\textit{Oryzias javanicus}$. The method used to solve this problem consists of 3 stages, namely the data preparation stage based on the model architecture, the model development stage, and the model selection stage through performance evaluation. The illustration of the method is depicted as in Figure ~\ref{fig: diagram-system}

Regarding Table \ref{tab: tablegap}, in this context, we are developing an identification and classification system for \textit{Oryzias celebensis} and \textit{Oryzias javanicus} fish using Deep Learning and Transfer Learning implemented on a modified MobileNetV2. We believe our innovative approach will have a significant impact on the identification and classification process. Our comprehensive methodology combines cutting-edge techniques, which we are confident can be adapted to various identification and classification challenges in industry-specific applications, particularly modern systems for identifying endangered endemic fish species.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}
In this research, the materials used include various tools and components essential for processing data and system implementation.

%\subsection{Materials used in this research include the following essential components :}
\subsection{Data Sources}
\begin{enumerate}
    \item Defining Dataset Requirements
    \begin{itemize}
        \item Dataset Specifications: Determine the required number of samples, minimum image resolution, and file format (JPEG, PNG, etc.).
        \item Data Sources: Combine data from captured videos and images from the Internet to improve species representation.
    \end{itemize}

    \item Acquiring Data from Captured Videos
    \begin{itemize}
        \item Video Recording: Capture fish in various lighting and aquatic environments.
        \item Video Preprocessing:
  
          \begin{itemize}
            \item Frame Extraction: Extract frames at specific intervals to avoid redundant data.
            \item Noise Reduction: Apply filtering techniques like Gaussian blur or histogram equalization.
            \item Segmentation \& Tracking: Use background subtraction or object tracking (SORT, DeepSORT) to highlight fish in frames.
        \end{itemize}
    \end{itemize}
    
   \begin{itemize}
        \item Image Annotation:
        \begin{itemize}
            \item Utilize bounding box or mask segmentation with tools like LabelImg or Roboflow.
            \item Save annotations in YOLO (.txt) or COCO (.json) format based on model requirements.
        \end{itemize}
    
    \end{itemize}
\end{enumerate}


%The dataset used in this study is entirely primary data taken from the results of capturing fish objects moving freely in the aquarium. All Medaka fish images were taken from an aquarium containing various types of small freshwater fish in varying numbers located in the Biotechnology Laboratory, Department of Biology, Faculty of Mathematics and Natural Sciences, Hasanuddin University. Some captured images in four different background colors are depicted in Figure.~\ref{fig: Oryzias}. 
%The classified fish species are \textit{Oryzias celebensis} and \textit{Oryzias javanicus} which are the ancestors of all the medaka fish in Indonesia. Both fish are endangered and serious conservation efforts are needed to prevent the extinction of this rare fish. In front of the aquarium, four rectangular rooms with flat widths were built as a place to take pictures when the fish passed by, were active or stopped in the room. The rooms were given different background colors, that is, red, black, blue, and green. Above them were placed lights in such a way that the medaka fish could be clearly seen during the photo shoot, even though their size was relatively small. 

\begin{figure}[h]
    \includegraphics[width=14cm]{Images/Oryzias.png}
    \caption{Captured Oryzias in four different background colors.
    \label{fig: Oryzias}}
\end{figure}
 
\subsection{Research Methodology}
We followed a systematic approach to develop an object detection and classification system for images, specifically targeting the identification of Oryzias celebensis and Oryzias javanicus. Our process includes key steps such as \textbf{Image Processing}—preprocessing and enhancing raw image data to improve model training, as well as \textbf{Building Block Architecture} building a robust framework by implementing architectures like YOLO and MobileNetV2, combined with transfer learning to enhance efficiency. Additionally, we developed a customised YOLO-based novelty model, called \textbf{YOLO-Medaka}, fine-tuned specifically for precise recognition of medaka fish.

While several researchers have employed MobileNetV2 and its variants—modified with various techniques—for applications such as mask detection, rust disease classification in plants, melanoma cancer detection, and Cicer arietinum variety classification, our primary focus is on the automatic detection of Oryzias celebensis and Oryzias javanicus. These species are particularly significant as they are endangered in Indonesia, and our tailored approach aims to address the unique challenges involved in their identification. The methods and techniques we used are further explained in Figure \ref{fig: YOLO-Medaka-diagram}


\begin{figure}[h]
\includegraphics[width=11 cm]{Images/YOLOv8-Medaka Workflow Diagram (1).jpg}
\caption{The YOLOv8-Medaka Workflow Diagram.
\label{fig: YOLO-Medaka-diagram}}
\end{figure}


\subsection{Image Processing}

\subsubsection{Rescaling}
The captured images of medaka fish will be rescaled to a square size of 224x224 pixels. Two rescaling methods will be applied: Padding and Non-Padding. The Padding method adds a uniform background color to both sides of the image if it's too small. Meanwhile, the Non-Padding method rescales the image directly to fit the square dimensions. All this process can be seen in Figure \ref{fig: padding} 


\subsubsection{Normalization}
Color value normalization is performed on each pixel by dividing it by 255, as shown in Figure \ref{fig: normalization}.
\begin{figure}[h]
\includegraphics[width=7 cm]{Images/normalization.png}
\caption{Color Value Normalization for Each Pixel.
\label{fig: normalization}}
\end{figure}

\subsubsection{Augmentation}
Ada dua metode augmentasi yang digunakan yaitu: shift dan rotasi. Randomly shift the image left/right by $20\%$ of its width, as shown in Figure \ref{fig: rotate}.

\begin{figure}[h]
\includegraphics[width=7 cm]{Images/rotate.png}
\caption{Width shift and height shift of 0.2 ($20\%$).
\label{fig: rotate}}
\end{figure}

\subsubsection{Image Dataset Splitting}
There are 661 images of \textit{Oryzias celebensis}, divided into 456 ($70\%$) for training, 146 ($20\%$) for validation, and 59 ($10\%$) for testing. There are 886 images of \textit{Oryzias javanicus}, split into 612 ($80\%$) for training, 177 ($20\%$) for validation, and 97 ($10\%$) for testing. An example image can be seen in Figure \ref{fig: Oryzias}.

\subsection{Building Block Architecture}

%\subsubsection{YOLOv8}
%The YOLOv8 architecture is designed to improve speed, accuracy, and ease of use over its predecessors (YOLOv5, YOLOv7). We use YOLOv8 as the main architecture model in this research, some components will be combined with MobileNetv2 and ANN-MLP architecture models. While YOLOv8 has several important components namely:

%\begin{itemize}
%    \item Backbone Network. The network serves to extract hierarchical features from the %input image, providing a comprehensive representation of the visual information.
%    \item Neck Architecture. The neck structure is responsible for feature fusion,  %combining multi-scale information and improving the model’s ability to detect objects of %varying sizes. 
%    \item YOLO Head. This component generates predictions based on the features extracted %by the backbone network and the neck architecture.
%\end{itemize}

%\begin{figure}[h]
%\includegraphics[width=10 cm]{Definitions/Architecture of YOLOv8.jpg}
%%%\caption{YOLOv8 Architecture.
%%\label{fig: YOLOv8}}
%\end{figure}
 
\subsubsection{MobileNetV2}
This architecture will be combined with several components in the backbone component of YOLOv8. This is done to improve the performance of hierarchical image feature extraction. Based on MobileNetv2 architecture and features, let’s looks the component and its steps.
\begin{itemize}
    \item Data Preparation. This involves preprocessing the images, splitting the dataset into training and validation sets, and applying data augmentation techniques to improve the model’s generalization ability.
    \item Transfer Learning. For initializing the model with pre-trained weights, the training process can be accelerated, and the model can benefit from the knowledge learned from the source dataset.
    \item Fine-tuning. This process Involves training the model on a target dataset while keeping the pre-trained weights fixed for some layers.
    \item Hyperparameter Tuning. Play a role in optimizing the performance of MobileNetV2. Carefully select parameters such as learning rate and regularization techniques to achieve the best possible results.
\end{itemize}

\begin{figure}[h]
\includegraphics[width=3 cm]{Images/Main-MobileNetv2-Diagram.jpg}
\caption{MobileNetV2 Architecture.
\label{fig: MobileNetV2}}
\end{figure}

\subsubsection{VGG16}
Based on the VGG-16 architecture, it is illustrated below in detail:
\begin{itemize}
    \item Input Layer. Input dimensions: (224, 224, 3). 
    \item Convolutional Layers (64 filters, 3×3 filters, same padding).
    \item Max Pooling Layer (2×2, stride 2).
    \item Convolutional Layers (128 filters, 3×3 filters, same padding). 
    \item Max Pooling Layer (2×2, stride 2). Max-pooling layer with a pool size of 2×2 and a stride of 2.
    \item Convolutional Layers (256 filters, 3×3 filters, same padding).
    \item Convolutional Layers (512 filters, 3×3 filters, same padding).
    \item Max Pooling Layer (2×2, stride 2).
    \item Stack of Convolutional Layers and Max Pooling Two additional convolutional layers after the previous stack.
    \item Flattening. Flatten the output feature map (7x7x512) into a vector of size 25088.
    \item Fully Connected Layers. Three fully connected layers with ReLU activation. First layer with input size 25088 and output size 4096. Second layer with input size 4096 and output size 4096. Third layer with input size 4096 and output size 1000, corresponding to the 1000 classes in the ILSVRC challenge. Softmax activation is applied to the output of the third fully connected layer for classification.
\end{itemize}

\begin{figure}[h]
\includegraphics[width=3 cm]{Images/Mind-VGG16-architecture.jpg}
\caption{Visual Geometry Group (VGG16) Architecture.
\label{fig: MobileNetV2}}
\end{figure}

\subsubsection{Transfer Learning}
   \begin{itemize}
     \item Load MobileNetV2 model.
     \item Freeze all layers.
     \item Create a Top Model with the following sequence: 
     \[ Flatten,Dense^\rightarrow(x_1),Dense^\rightarrow(x_2), Dropout(\rho),SoftMax.\]     
     \item After constructing the Top Model's output layer, fine-tuning is performed by freezing the first X layers, then fitting the model with these parameters: Total Epochs, Batch Size, and Steps per Epoch.
    \end{itemize}

\subsection{Computational Environment}
In this research project, the process of forming deep learning and transfer learning models uses the DIKTI AI Center facility which uses NVIDIA technology with a computing capacity of 25 PetaFLOPS. The supercomputer facility consists of five NVIDIA DGX A100 server machine nodes. Each node has a dual AMD Rome CPU with eight graphics processing units (GPUs) with multi-instance GPU (MIG) capabilities: 4 GPU @ 40GB, Processor 8 Core, and RAM 64 GB. 
Each server node is equipped with 1TB of RAM and 5TB of high-speed NVME storage, with a total processing power of 5 TeraFLOPS per node. The supercomputer nodes are interconnected via a high-speed Mellanox network, with NVLink links between the five units, each of which has 8 Core GPUs. The procurement of the DIKTI AI Center supercomputer facility is used to strengthen the creation of national AI talent through various training and education activities in collaboration with industry. This facility can be used by educational institutions, including Hasanuddin University, to facilitate the development of AI technology innovations to meet the needs of industry and society. The software used is Jupiter Notebook, TensorFlow, Roboflow, and OpenCV cuda version 11.4.


\subsection{Performance Measurements}
After the model fitting process, performance evaluation is conducted using two scenarios. Based on the Confusion Matrix.

    \begin{figure}[h]
    \includegraphics[width=6 cm]{Images/Confusion Matrix.png}
    \caption{Confusion Matrix.
    \label{fig: confusion}}
    \end{figure}     
    
    
     \begin{description}
        \item \[Sensitivity \text{ }(recall) = \frac{TP}{TP+FN}\]
        \item \[Accuracy = \frac{TP+TN}{TP+FN+FP+FN}\] 
        \item \[Precision = \frac{TP}{TP+FP}\]
        \item \[F_n-Score = (n+1)\times\frac{Precision \times Recall}{Precision + Recall}\]
    \end{description}
    
        Area Under the Curve - Receiver Operating Characteristic (AUC-ROC). 
        \begin{figure}[h]
            \includegraphics[width=6 cm]{Images/AUC-ROC.png}
            \caption{The AUC-ROC curve}
            \label{fig: AUC-ROC}
        \end{figure}
            %\texttt{enumerate} environment
    


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Novel MobileNet-Medaka}
YOLO-medaka was built from the idea of modifying YOLOv8 by performing several combinations of algorithms, such as MobileNetV2 and VGG16, for the identification and classification of endemic medaka fish with Types: Oryzias celebensis and Oryzias javanicus. this endemic medaka has been declared endangered by the International Union for Conservation of Nature and Natural Resources (IUCN). Therefore, to prevent this, this research was conducted, it is expected to provide a significant impact and major contribution to the conservation of endemic species, especially this endemic fish, so that it does not become extinct. A general overview of the system is shown in Figure \ref{fig: diagram-system}.


\begin{figure}[h]
    \includegraphics[width=7 cm]{Images/diadram-system-Medaka-Model.jpg}
    \caption{A General Diagram System.
    \label{fig: diagram-system}}
    \end{figure}

\begin{figure}[h]
\includegraphics[width=8 cm]{Images/Modified-MobileNetV2-Architecture.jpg}
\caption{Modified MobileNetV2 Architecture.
\label{fig: Modified-MobileNetV2-Architecture}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discuss}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Captured Images}
\subsection{Image Pre-processing}
The medaka fish image data that was successfully captured consisted of 661 and 886 images for Oryzias celebensis and Oryzias javanicus, respectively. 


%\subsection{Rescaling Images}
After obtaining the image dataset, each image was padded to achieve a 1:1 aspect ratio by adding pixels to the shorter side using colors similar to the image's edge. This technique is rarely, if ever, used by other researchers, making it a unique approach in this study.
After adjusting the images to a 1:1 ratio, they were then resized to 224 x 224 pixels. The results can be seen in Figure \ref{fig: padding}.

\begin{figure}[h]
    \includegraphics[width= 11cm]{Images/Padding01.jpg}
    \caption{Comparison of Non-Padding and Padding Resize mechanisms.
    \label{fig: padding}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The performance comparison of each model on both padded and non-padded datasets is presented in Table \ref{tab: perform}, highlighting the differences in accuracy and effectiveness between the two approaches.

\begin{table}[H]
\caption{Accuracy of padding and non-padding images.\label{tab: perform}}
	\begin{adjustwidth}{-\extralength}{0cm}
		\newcolumntype{C}{>{\centering\arraybackslash}X}
		\begin{tabularx}{\fulllength}{CCCCCC}
			\toprule
\textbf{Model}& \textbf{Image}& \textbf{Sensitivity}& \textbf{Precision} & \textbf{F1 Score} & \textbf{Accuracy}\\
			\midrule
\multirow[m]{1}{*}{MobileNetV2}	& Non-padding& 98.2 & 93.8	& 96 & 96.95\\
                   \midrule
\multirow[m]{1}{*}{MobileNetV2}  & padding	& 96.3	& 87.6	& 91.93& 93.9\\
                   \midrule
\multirow[m]{1}{*}{P-MobileNetV2} & Non-padding& 79& 98.4	& 87.6	& 89\\
                  \midrule
\multirow[m]{1}{*}{P-MobileNetV2} & padding	&98.46 &98.46	&98.46	&98.78\\
                   \midrule
\multirow[m]{1}{*}{VGG16} & Non-padding	& 95.3	& 93.5	& 94.5	& 95.7\\
                   \midrule
\multirow[m]{1}{*}{VGG16}   & padding	& 97.2	& 96.2	& 81	& 87\\
                   \midrule
\multirow[m]{1}{*}{P-VGG16} & Non-padding	& 63.3	& 98.4	& 77	& 76\\
                   \midrule
\multirow[m]{1}{*}{P-VGG16} & padding	&92.7 &98.4	&96.3	&96.3\\   
   \bottomrule
		\end{tabularx}
	\end{adjustwidth}
\end{table}

P-MobileNetV2 refers to the model trained using the padded dataset, where images were adjusted to a 1:1 aspect ratio to enhance classification performance and consistency during training.

We have obtained graphical results that compare the ROC accuracy of MobileNetV2 with the modified P-MobileNetV2, along with VGG16 and the modified P-VGG16. These comparisons are clearly shown in the following graph :

\begin{figure}[h]
\includegraphics[width=12 cm]{Images/Accuracy dan ROC Model MobileNetV2.png}
\caption{Diagram Accuracy dan ROC Model MobileNetV2.
\label{fig: diagram Accuracy dan ROC Model MobileNetV2}}
\end{figure}

The Accuracy and ROC diagrams of the MobileNetV2 model illustrate its performance in classification tasks. The accuracy graph shows the model's learning progress over time, while the ROC (Receiver Operating Characteristic) curve evaluates its ability to distinguish between classes, highlighting its sensitivity and specificity at various thresholds.


\begin{figure}[h]
\includegraphics[width=12 cm]{Images/Accuracy dan ROC Model P-MobileNetV2.png}
\caption{Diagram Accuracy dan ROC Model P-MobileNetV2.
\label{fig: diagram Accuracy dan ROC Model P-MobileNetV2}}
\end{figure}

The "Accuracy and ROC Diagram of P-MobileNetV2 Model" illustrates the model's performance metrics that were modified, including classification accuracy and the Receiver Operating Characteristic (ROC) curve, evaluating its predictive capability.

\begin{figure}[h]
\includegraphics[width=12 cm]{Images/Accuracy dan ROC Model VGG16.png}
\caption{Diagram Accuracy dan ROC Model VGG16.
\label{fig: diagram Accuracy dan ROC Model VGG16}}
\end{figure}

The diagram presents the accuracy and Receiver Operating Characteristic (ROC) curve of the VGG16 model, illustrating its classification performance and ability to distinguish between classes based on true positive and false positive rates during the evaluation process.

\begin{figure}[h]
\includegraphics[width=12 cm]{Images/Accuracy dan ROC P-VGG16.png}
\caption{diagram Accuracy dan ROC Model P-VGG16.
\label{fig: diagram Accuracy dan ROC P-VGG16}}
\end{figure}

The diagram displays the accuracy and Receiver Operating Characteristic (ROC) curve of the modified P-VGG16 model, highlighting its improved classification performance and enhanced capability to differentiate between classes based on true positive and false positive rates during the evaluation phase.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this research, we successfully developed a MobileNetV2 architecture model using transfer learning and dataset padding to classify \textit{Oryzias Celebensis} and \textit{Oryzias Javanicus}. We added several layers to the Classification Layer consisting of 5 layers: Flatten, two Dense layers with ReLU activation functions (1024 and 512 neurons respectively), a Dropout layer with 0.2 rate, and a final Dense layer with two neurons using SoftMax activation. This modification allowed us to leverage features learned by MobileNetV2 with ImageNet weights while adapting the model for our specific classification task.

The results demonstrate that this approach effectively enhances classification performance. The additional dense layers serve as feature extraction and classifier layers, mapping the features extracted by MobileNetV2 into the two target classes Oryzias celebensis and Oryzias javanicus. The Dropout layer (rate 0.2) helps prevent overfitting, ensuring the model maintains strong generalisation on new data. The SoftMax activation function in the final dense layer ensures the model's output can be interpreted as class probabilities, simplifying the final result interpretation. This structure improves accuracy while maintaining robustness, making the model reliable for distinguishing between the two fish species.

Further discussion reveals that using MobileNetV2 as the base model offers several advantages. (1) MobileNetV2 is specifically designed for mobile devices, making it lightweight and efficient for deployment in mobile applications or resource-constrained environments. (2) By employing dataset padding and transfer learning, we leveraged the pre-trained knowledge of MobileNetV2 from large-scale datasets, significantly accelerating and simplifying the model training process. This approach allows the model to achieve high accuracy with limited computational resources. Additionally, the added dense layers enhance feature extraction and classification, while the dropout layer ensures robustness against overfitting. The SoftMax activation in the final layer provides interpretable probability outputs for each target class (\textit{Oryzias celebensis} and \textit{Oryzias javanicus}). The complete architecture of the developed model is illustrated in Figure \ref{fig: Modified-MobileNetV2-Architecture}.

%\begin{figure}[h]
%\includegraphics[width=6 cm]{Definitions/Modified-MobileNetV2-Architecture.jpg}
%\caption{Modified MobileNetV2 Architecture.
%\label{fig: Modified-MobileNetV2-Architecture}}
%\end{figure}

We have evaluated the performance of the MobileNetV2 architecture using additional layers configured as described in the system overview, and applied padding to the dataset in order to classify the fish species \textit{Oryzias celebensis} and \textit{Oryzias javanicus} using a limited dataset. The evaluation was conducted using tests based on the confusion matrix and the ROC-AUC curve, utilizing two types of dataset: a nonpadded dataset and a padded dataset. Two model architectures were used for comparison, MobileNetV2 and VGG16. The results show that MobileNetV2 performed better on the padded dataset compared to its performance on the nonpadded dataset, as well as compared to VGG16 on both dataset types.

%In this evaluation, we utilized the Kubeflow Dikti AI platform, which offers computing specifications as detailed in Table \ref{tab: spectab}. This platform provided the necessary environment for efficient model training and testing. For software tools, we employed Python as the main programming language, along with OpenCV for image processing, TensorFlow for building and training deep learning models, and Jupyter Notebook as the development interface. Together, these tools supported the implementation and evaluation of our classification models for \textit{Oryzias celebensis} and \textit{Oryzias javanicus} using both padded and non-padded datasets.
  
%\unskip

%\begin{table}[H]
%\caption{Gap description in building research motivation.\label{tab: spectab}}
%	\begin{adjustwidth}{-\extralength}{5mm}
%		\newcolumntype{C}{>{\centering\arraybackslash}X}
%		\begin{tabularx}{\fulllength}{m{5cm} m{3cm} m{8cm}}
%			\toprule
%            \textbf{No.}& \textbf{Item}& \textbf{SubItem}\\
%			\midrule
%\multirow[m]{0.5}{*}{1} & Software & 4 x GPU @ 40 GB,
%                                     Processor 8 Core,
%                                     Ram 64 GB	\\
%                   \midrule
%\multirow[m]{0.5}{*}{2} & Hardware & Python,
%                                    OpenCV,
%                                    TensorFlow,
%                                    Jupiter Notebook \\
%                     \bottomrule
%		\end{tabularx}
%	\end{adjustwidth}
%\end{table}





%Authors should discuss the results and how they can be interpreted from the perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

Based on Table 2, the P-MobileNetV2 model has better evaluation performance results compared to the other models, with Sensitivity = 98.46, Precision = 98.46, F1 Score = 98.46, and Accuracy = 98.7. In this study, we have also applied a padding technique to our dataset. This technique involves adding artificial border values to cover empty spaces, ensuring that the images remain square (1:1 aspect ratio), which helps preserve spatial information during the convolution process. As a result, the P-MobileNetV2 and P-VGG16 models trained on the padded dataset achieved higher metrics compared to models trained without dataset padding, demonstrating that this technique can effectively improve model performance. This can be observed in Figures 10 and 12, where both graphs show notably better trends than those in Figures 9 and 11.

Furthermore, our research provides insight that the padding technique not only improves model accuracy but also helps maintain consistency and preserves spatial context in the dataset images. This is particularly crucial for tasks such as segmentation and object detection, where spatial information in images or datasets is highly important. Therefore, the implementation of the padding technique in this study has proven to be a critical step in enhancing model performance and achieving strong results in our classification task.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Patents}

%This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, A.L. and M.K.; methodology, A.L.; software, A.L. and M.K.; validation, A.L., I.A., A.I.B., and M.K.; formal analysis, A.L., I.A., and A.I.B.; investigation, I.A, and A.I.B; resources, A.L. and I.A.; data curation, A.L., I.A., and A.I.B; writing-original draft preparation, A.L. and M.K.; writing-review and editing, A.L., I.A and M.K.; visualization, A.L.; supervision, M.K.; project administration, A.L., and I.A.; funding acquisition, A.L. All authors have read and agreed to the published version of the manuscript.}

%\href{http://img.mdpi.org/data/contributor-role-instruction.pdf}{CRediT taxonomy} for the term explanation. Authorship must be limited to those who have contributed substantially to the work~reported.}

%\funding{Please add: ``This research received no external funding'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.}

%\institutionalreview{In this section, you should add the Institutional Review Board Statement and approval number, if relevant to your study. You might choose to exclude this statement if the study did not require ethical approval. Please note that the Editorial Office might ask you for further information. Please add “The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving humans. OR “The animal study protocol was approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving animals. OR “Ethical review and approval were waived for this study due to REASON (please provide a detailed justification).” OR “Not applicable” for studies not involving humans or animals.}

%\informedconsent{Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.

%Written informed consent for publication must be obtained from participating patients who can be identified (including by the patients themselves). Please state ``Written informed consent has been obtained from the patient(s) to publish this paper'' if applicable.}

%\dataavailability{We encourage all authors of articles published in MDPI journals to share their research data. In this section, please provide details regarding where data supporting reported results can be found, including links to publicly archived datasets analyzed or generated during the study. Where no new data were created, or where data is unavailable due to privacy or ethical restrictions, a statement is still required. Suggested Data Availability Statements are available in section ``MDPI Research Data Policies'' at \url{https://www.mdpi.com/ethics}.} 

% Only for journal Nursing Reports
%\publicinvolvement{Please describe how the public (patients, consumers, carers) were involved in the research. Consider reporting against the GRIPP2 (Guidance for Reporting Involvement of Patients and the Public) checklist. If the public were not involved in any aspect of the research add: ``No public involvement in any aspect of this research''.}

% Only for journal Nursing Reports
%\guidelinesstandards{Please add a statement indicating which reporting guideline was used when drafting the report. For example, ``This manuscript was drafted against the XXX (the full name of reporting guidelines and citation) for XXX (type of research) research''. A complete list of reporting guidelines can be accessed via the equator network: \url{https://www.equator-network.org/}.}

\acknowledgments{The author sincerely thanks Hasanuddin University for its generous support, which helped make this research possible and enabled it to be published in an international journal.}

\conflictsofinterest{The author declares no conflict of interest in preparing this paper, including any personal or institutional relationships that could have influenced the content, analysis, or conclusions presented herein.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
%\sampleavailability{ authors.}

%% Only for journal Encyclopedia
%\entrylink{ platform.}

%\abbreviations{Abbreviations}{


%\noindent 
%\begin{tabular}{@{}ll}
%MDPI & Multidisciplinary Digital Publishing Institute\\
%DOAJ & Directory of open access journals\\
%TLA & Three letter acronym\\
%LD & Linear dichroism
%\end{tabular}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
%\appendixtitles{
%\section[\appendixname~\thesection]{}
%\subsection[\appendixname~\thesubsection]{}


%\begin{table}[H] 

%\section[\appendixname~\thesection]{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
%\bibliography{your_external_BibTeX_file}

%=====================================
% References, variant B: internal bibliography
%=====================================
%\begin{thebibliography}{999}

\bibliography{mybibliography}
% Reference 1
%\bibitem[Author1(year)]{ref-journal}
%Author~1, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142--149.
% Reference 2
%\bibitem[Author2(year)]{ref-book1}
%Author~2, L. The title of the cited contribution. In {\em The Book Title}; Editor 1, F., Editor 2, A., Eds.; Publishing House: City, Country, 2007; pp. 32--58.
% Reference 3
%\bibitem[Author3(year)]{ref-book2}
%Author 1, A.; Author 2, B. \textit{Book Title}, 3rd ed.; Publisher: Publisher Location, Country, 2008; pp. 154--196.
% Reference 4
%\bibitem[Author4(year)]{ref-unpublish}
%Author 1, A.B.; Author 2, C. Title of Unpublished Work. \textit{Abbreviated Journal Name} year, \textit{phrase indicating stage of publication (submitted; accepted; in press)}.
% Reference 5
%\bibitem[Author5(year)]{ref-communication}
%Author 1, A.B. (University, City, State, Country); Author 2, C. (Institute, City, State, Country). Personal communication, 2012.
% Reference 6
%\bibitem[Author6(year)]{ref-proceeding}
%Author 1, A.B.; Author 2, C.D.; Author 3, E.F. Title of presentation. In Proceedings of the Name of the Conference, Location of Conference, Country, Date of Conference (Day Month Year); Abstract Number (optional), Pagination (optional).
% Reference 7
%\bibitem[Author7(year)]{ref-thesis}
%Author 1, A.B. Title of Thesis. Level of Thesis, Degree-Granting University, Location of University, Date of Completion.
% Reference 8
%\bibitem[Author8(year)]{ref-url}
%Title of Site. Available online: URL (accessed on Day %Month Year).
%\end{thebibliography}

% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
\end{adjustwidth}
\end{document}

