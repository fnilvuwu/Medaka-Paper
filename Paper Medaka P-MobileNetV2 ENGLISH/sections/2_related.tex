In this study, we employ MobileNetV2 for image-based object classification. This model is an enhanced version of the earlier MobileNetV1 architecture. MobileNetV2 is a Convolutional Neural Network (CNN) specifically designed for resource-constrained devices, such as smartphones, IoT devices, and embedded systems. Developed by Google AI and published in 2018 \cite{8578572,akay2021deep}, it offers several advantages highlighted in various studies, including lightweight architecture, competitive performance, and compatibility with multiple deep learning frameworks. The model incorporates inverted residual blocks with linear bottlenecks, improving efficiency while maintaining accuracy. These features make MobileNetV2 suitable for real-time applications on low-power devices. The general structure of the MobileNetV2 model, as illustrated in the framework, can be seen in Figure \ref{fig: MobileNetV2 Architecture}. 

\begin{figure}[h]
\includegraphics[width=1\textwidth]{Images/MobileNetV2.png}
\caption{General MobileNetV2 Architecture.
\label{fig: MobileNetV2 Architecture}}
\end{figure}

In study \cite{golcuk2023classification}, images of Cicer arietinum (chickpea) varieties were input into two architectures. The first used transfer learning with fine-tuning on a pre-trained MobileNetV2 CNN model for classification. The second was a hybrid architecture incorporating Long Short-Term Memory (LSTM) layers to account for temporal features. The results showed $92.3\%$ accuracy for the first model and $92.97\%$ for the hybrid, demonstrating a high success in classifying chickpea images. Additionally, authors \cite{sen2021face} applied a deep learning approach using MobileNetV2 within a PyTorch and OpenCV Python framework for mask detection during COVID-19. Their model efficiently identified proper mask usage, highlighting MobileNetV2's adaptability for diverse image-based tasks, from agricultural classification to public health applications. These studies underscore its effectiveness in achieving high accuracy while maintaining computational efficiency for real-world deployments.

In another previous study\cite{gulzar2023fruit}, the authors introduced customized heads comprising five different layers into the MobileNetV2 architecture. The model’s original classification layers were replaced with these customized heads, resulting in a modified version called TL-MobileNetV2. This adaptation achieved an accuracy of $99\%$ $—3\%$ higher than standard MobileNetV2, while maintaining a minimal error rate of just $1\%$. When compared to AlexNet, VGG16, InceptionV3, and ResNet, TL-MobileNetV2 demonstrated superior performance. Meanwhile, authors \cite{indraswari2022melanoma} implemented MobileNetV2 for melanoma cancer classification, reporting an accuracy of up to $85\%$ on the ISIC-Archive dataset, outperforming models like ResNet50V2, InceptionV3, and InceptionResNetV2. Another study \cite{shahoveisi2023application} evaluated four convolutional neural networks—Xception, ResNet50, EfficientNetB4, and MobileNet—for detecting rust disease in three commercially important field crops. Results showed EfficientNetB4 as the most accurate (average accuracy = $94.29\%$), followed by ResNet50 ($93.52\%$). Though MobileNetV2 trailed slightly in this comparison, its computational efficiency makes it viable for resource-constrained applications, reinforcing its versatility across medical and agricultural domains. Further research \cite{hamid2022smart} uses MobileNetV2, a deep learning convolutional neural network (DCNN) for seed classification. A total of 14 different classes of seeds were used for the experimentation. The results indicate accuracies of $98\%$ and $95\%$ on training and test sets, respectively.

\begin{table}[H]
\caption{Gap description in building research motivation.\label{tab: tablegap}}

	\begin{adjustwidth}{-\extralength}{0cm}
		\newcolumntype{C}{>{\centering\arraybackslash}X}
		\begin{tabularx}{\fulllength}{m{5mm} m{8.5cm} m{8.5cm}}
			\toprule
            \textbf{No.}& \textbf{Previous Study}& \textbf{Current Study}\\
			\midrule
\multirow[m]{0.5}{*}{1} & 
The hybrid architecture incorporates Long Short-Term Memory (LSTM) layers, which also account for temporal data features in classification. \cite{golcuk2023classification} & 
The image data features four different background colors: red, black, blue, and green, with overhead lighting to ensure the fish are clearly visible.\\
                   \midrule
\multirow[m]{0.5}{*}{2}  & Implementation of a deep learning approach using the MobileNetV2 framework, integrated with PyTorch and OpenCV in Python, for mask detection during COVID-19. \cite{sen2021face}	& We developed this application using Jupyter Notebook, TensorFlow, Roboflow, and OpenCV for the identification of \textit{Oryzias celebensis} and \textit{Oryzias javanicus}.\\
                   \midrule
\multirow[m]{0.5}{*}{3} & Implementing MobileNetV2 for Melanoma Cancer Classification. MobileNetV2 demonstrates higher accuracy compared to ResNet50V2, InceptionV3, and InceptionResNetV2. \cite{indraswari2022melanoma}& Development of a MobileNetV2 architecture using a transfer learning approach with dataset padding and additional layers for the Classification Layer. This layer consists of 5 components: a Flatten layer, two Dense layers with ReLU activation functions.\\
                  \midrule
\multirow[m]{0.5}{*}{4} & With utilises MobileNetV2, a deep learning convolutional neural network (DCNN), for seed classification. \cite{hamid2022smart} & We developed modified versions: P-MobileNetV2 (from MobileNetV2) and P-VGG16 (from VGG16) to evaluate the effects of our architectural changes.\\                     \bottomrule
		\end{tabularx}
	\end{adjustwidth}
\end{table}

Regarding Table \ref{tab: tablegap}, in this context, we are developing an identification and classification system for \textit{Oryzias celebensis} and \textit{Oryzias javanicus} fish using Deep Learning and Transfer Learning implemented on a modified MobileNetV2. We believe our innovative approach will have a significant impact on the identification and classification process. Our comprehensive methodology combines cutting-edge techniques, which we are confident can be adapted to various identification and classification challenges in industry-specific applications, particularly modern systems for identifying endangered endemic fish species.
